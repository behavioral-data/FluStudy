{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f632b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f467ba",
   "metadata": {},
   "source": [
    "Checking for equality between two tuples that include tensors cannot take advantage of the == operator. Check the following example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88ad7b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_103640/3888505424.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#also False! 'in' set operator cannot be used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobs2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobs3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch \n",
    "import os\n",
    "\n",
    "directo = '/homes/gws/jacopo/trelium/SeattleFluStudy/debugbatch/2'\n",
    "filename = 'debug_batch_2.p'\n",
    "\n",
    "with open(os.path.join(directo, filename), 'rb') as fp:\n",
    "    data = pickle.load(fp)\n",
    "    obs1 = (data['inputs_embeds'][0], data['label'][0], data['participant_id'][0], data['id'][0], data['end_date_str'][0])\n",
    "    obs2 = (data['inputs_embeds'][1], data['label'][1], data['participant_id'][1], data['id'][1], data['end_date_str'][1])\n",
    "    obs3 = (data['inputs_embeds'][0], data['label'][0], data['participant_id'][0], data['id'][0], data['end_date_str'][0])    \n",
    "    \n",
    "    print(hash(obs1) == hash(obs2))  #False \n",
    "    print(hash(obs1) == hash(obs3))  #also False! 'in' set operator cannot be used\n",
    "        \n",
    "    print(obs1 == obs2)\n",
    "    print(obs1 == obs3)\n",
    "    \n",
    "    #https://stackoverflow.com/questions/8705378/pythons-in-set-operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11ff7880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "obss = set()\n",
    "obss.add(obs2)\n",
    "obss.add(obs3) #don't add obs1\n",
    "\n",
    "\n",
    "\n",
    "print(obs1 in obss)\n",
    "print(hash(obs1) == hash(obs3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0248e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "count = Counter()\n",
    "for epoch in allresults.keys():\n",
    "    count.update(allresults[epoch]['files_in_epoch'])\n",
    "\n",
    "print(len(count))          \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88574d",
   "metadata": {},
   "source": [
    "We need to define a custom function to check for equality item-to-item in our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35af68e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_equality(observation, other):\n",
    "    \"\"\"Returns true if two given example tuples contain tensors with same shape and numbers \n",
    "    as well as equal elements in other fields.\"\"\"\n",
    "    equality = []\n",
    "    for i in range(len(observation)):\n",
    "        if torch.is_tensor(observation[i]):\n",
    "            #if torch.eq(observation[i], other[i]).sum() == torch.numel(observation[i]):\n",
    "            if torch.equal(observation[i], other[i]):\n",
    "                equality.append(True)\n",
    "        elif observation[i] == other[i]:\n",
    "            equality.append(True)\n",
    "    if len(equality) == 5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "class HashTensorWrapper():\n",
    "    \"\"\"provides rudimental hashing support for tensors\n",
    "    https://discuss.pytorch.org/t/how-to-put-tensors-in-a-set/123836/6\n",
    "    \"\"\"\n",
    "    def __init__(self, tensor):\n",
    "        self.tensor = tensor\n",
    "        self.hashcrap = torch.arange(self.tensor.numel(), device=self.tensor.device).reshape(self.tensor.size())\n",
    "\n",
    "    def __hash__(self):\n",
    "        if self.hashcrap.size() != self.tensor.size():\n",
    "            self.hashcrap = torch.arange(self.tensor.numel(), device=self.tensor.device).reshape(self.tensor.size())\n",
    "        return hash(torch.sum(self.tensor*self.hashcrap))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return torch.all(self.tensor == other.tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4384d5",
   "metadata": {},
   "source": [
    "Let's now chech wheter there are epochs that contain duplicate items (examples).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5070fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "allresults = {}\n",
    "\n",
    "base_dir = '/homes/gws/jacopo/trelium/SeattleFluStudy/debugbatch'\n",
    "for (root,direcs,files) in os.walk(base_dir):\n",
    "    dirs = direcs\n",
    "    break\n",
    "\n",
    "for directory in dirs: #inspect each epoch (one per directory) \n",
    "    totalepochsize = 0\n",
    "    allepoch = set()\n",
    "    duplicates = []\n",
    "    for filename in os.listdir(os.path.join(base_dir, directory)): #inspect each batch\n",
    "        with open(os.path.join(base_dir,directory, filename), 'rb') as fp:\n",
    "            data = pickle.load(fp)\n",
    "            \n",
    "            batchsize = len(data['inputs_embeds']) #each embedding: torch.Size([5760, 8])\n",
    "            totalepochsize += batchsize\n",
    "            \n",
    "            for i in range(batchsize):\n",
    "                long = tuple() #transform data in different format\n",
    "                for key in data:\n",
    "                    long = long + (data[key][i],)\n",
    "                \n",
    "                if long in allepoch: #TODO this does not work, check each field for equlity \n",
    "                    duplicates.append(long)\n",
    "                else:\n",
    "                    allepoch.add(long)\n",
    "\n",
    "    allresults[directory] = {'totalepochsize' : totalepochsize, \n",
    "                            'duplicates' : duplicates,\n",
    "                            'files_in_epoch' : allepoch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8522ff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks whether some epochs contain duplicate examples. \n",
    "counts = dict()\n",
    "for epoch in allresults.keys():\n",
    "    repetitions = 0\n",
    "    for observationout in allresults[epoch]['files_in_epoch']: #within an epoch\n",
    "        for observationin in allresults[epoch]['files_in_epoch']: #within an epoch\n",
    "            if check_equality(observationin, observationout):\n",
    "                repetitions +=1\n",
    "    repetitions -= len(allresults[epoch]['files_in_epoch'])\n",
    "    counts[epoch] = repetitions\n",
    "    print(f'done epoch {epoch}')\n",
    "\n",
    "\n",
    "for i in allresults\n",
    "    print ('epoch {} contains {} total examples, of which {} are repeated'.format(i,allresults[i]['totalepochsize'],counts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c0a59",
   "metadata": {},
   "source": [
    "Let's take advantage of the previously defined HashTensorWrapper class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1df2e2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                 | 0/217 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Invalid magic number; corrupt file?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63624/4169529001.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m#data = pickle.load(fp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mbatchsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputs_embeds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#each embedding: torch.Size([5760, 8])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/seattleflustudy/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/seattleflustudy/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m     \u001b[0mprotocol_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol_version\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPROTOCOL_VERSION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid magic number; corrupt file?"
     ]
    }
   ],
   "source": [
    "allresults = {}\n",
    "device = torch.device('cpu') #where to deserialize tensors \n",
    "\n",
    "\n",
    "base_dir = '/homes/gws/jacopo/trelium/SeattleFluStudy/debugbatch_fulldata'\n",
    "for (root,direcs,files) in os.walk(base_dir):\n",
    "    dirs = direcs\n",
    "    break\n",
    "\n",
    "for directory in dirs: #inspect each epoch (one per directory) \n",
    "    totalepochsize = 0\n",
    "    allepoch = set()\n",
    "    duplicates = tuple()\n",
    "    for filename in tqdm(os.listdir(os.path.join(base_dir, directory))): #inspect each batch\n",
    "        with open(os.path.join(base_dir,directory, filename), 'rb') as fp:\n",
    "            #data = pickle.load(fp)\n",
    "            data = torch.load(fp, map_location = device, pickle_module = pickle)\n",
    "            \n",
    "            batchsize = len(data['inputs_embeds']) #each embedding: torch.Size([5760, 8])\n",
    "            totalepochsize += batchsize\n",
    "            \n",
    "            for i in range(batchsize):\n",
    "                long = tuple() #transform data in different format\n",
    "                for key in data:\n",
    "                    if torch.is_tensor(data[key][i]):\n",
    "                        #data[key][i].to(device)  \n",
    "                        newdatapoint = HashTensorWrapper(data[key][i])\n",
    "                        long = long + (newdatapoint,)\n",
    "                    else:\n",
    "                        long = long + (data[key][i],)\n",
    "                \n",
    "                if long in allepoch: \n",
    "                    duplicates = duplicates + (long,)\n",
    "                else:\n",
    "                    allepoch.add(long)\n",
    "\n",
    "    allresults[directory] = {'totalepochsize' : totalepochsize, \n",
    "                            'duplicates' : duplicates,\n",
    "                            'files_in_epoch' : allepoch}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "138a6170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "['0', '1', '2', '3']\n",
      "None\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "from src.utils import get_unused_gpus\n",
    "print('done')\n",
    "\n",
    "free_devices = get_unused_gpus()\n",
    "print(free_devices)\n",
    "print(os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "\n",
    "\n",
    "\n",
    "devices = free_devices[0]\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(devices)\n",
    "\n",
    "print(os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
