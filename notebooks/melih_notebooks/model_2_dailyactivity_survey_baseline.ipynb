{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/melih/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (6,8,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df_survey = pd.read_csv('gates_data_csv/daily_surveys_onehot.csv')\n",
    "df_baseline = pd.read_csv('gates_data_csv/baseline_screener_onehot.csv')\n",
    "df_lab = pd.read_csv('gates_data_csv/lab_results_with_triggerdate.csv')\n",
    "df_daily_activity = pd.read_csv('gates_data_csv/activity_daylvl.csv')\n",
    "\n",
    "df_survey['timestamp'] = pd.to_datetime(df_survey.timestamp)\n",
    "df_lab['trigger_datetime'] = pd.to_datetime(df_lab.trigger_datetime)\n",
    "\n",
    "df_daily_activity.rename(columns = {'date':'timestamp'},inplace = True)\n",
    "df_daily_activity['timestamp'] =  pd.to_datetime(df_daily_activity.timestamp).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab_positive = df_lab.loc[df_lab.result == 'Detected'][['participant_id','trigger_datetime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding flu positive information\n",
    "df_survey = pd.merge(df_survey , df_lab_positive, how='left',on = 'participant_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 527877 entries, 0 to 527876\n",
      "Data columns (total 188 columns):\n",
      " #   Column                                         Dtype         \n",
      "---  ------                                         -----         \n",
      " 0   timestamp                                      datetime64[ns]\n",
      " 1   participant_id                                 object        \n",
      " 2   have_flu                                       int64         \n",
      " 3   recovered_yn                                   int64         \n",
      " 4   first_report_yn                                int64         \n",
      " 5   t_recov_severity__q_recov_fever_0              int64         \n",
      " 6   t_recov_severity__q_recov_fever_1              int64         \n",
      " 7   t_recov_severity__q_recov_fever_2              int64         \n",
      " 8   t_recov_severity__q_recov_fever_3              int64         \n",
      " 9   t_recov_severity__q_recov_fever_skipped        int64         \n",
      " 10  med_rx_name__4_0                               int64         \n",
      " 11  med_rx_name__4_1                               int64         \n",
      " 12  med_rx_name__4_skipped                         int64         \n",
      " 13  med_otc_name__5_0                              int64         \n",
      " 14  med_otc_name__5_1                              int64         \n",
      " 15  med_otc_name__5_skipped                        int64         \n",
      " 16  t_difficulty__q_dailyact_0                     int64         \n",
      " 17  t_difficulty__q_dailyact_1                     int64         \n",
      " 18  t_difficulty__q_dailyact_2                     int64         \n",
      " 19  med_otc_name__4_0                              int64         \n",
      " 20  med_otc_name__4_1                              int64         \n",
      " 21  med_otc_name__4_skipped                        int64         \n",
      " 22  symptom_severity__q_fatigue_0                  int64         \n",
      " 23  symptom_severity__q_fatigue_1                  int64         \n",
      " 24  symptom_severity__q_fatigue_2                  int64         \n",
      " 25  symptom_severity__q_fatigue_3                  int64         \n",
      " 26  symptom_severity__q_fatigue_skipped            int64         \n",
      " 27  med_otc_name__1_0                              int64         \n",
      " 28  med_otc_name__1_1                              int64         \n",
      " 29  med_otc_name__1_skipped                        int64         \n",
      " 30  t_recov_severity__q_recov_fatigue_0            int64         \n",
      " 31  t_recov_severity__q_recov_fatigue_1            int64         \n",
      " 32  t_recov_severity__q_recov_fatigue_2            int64         \n",
      " 33  t_recov_severity__q_recov_fatigue_3            int64         \n",
      " 34  t_recov_severity__q_recov_fatigue_skipped      int64         \n",
      " 35  symptom_severity__aches_q_0                    int64         \n",
      " 36  symptom_severity__aches_q_1                    int64         \n",
      " 37  symptom_severity__aches_q_2                    int64         \n",
      " 38  symptom_severity__aches_q_3                    int64         \n",
      " 39  symptom_severity__aches_q_skipped              int64         \n",
      " 40  t_recov_severity__q_recov_cough_0              int64         \n",
      " 41  t_recov_severity__q_recov_cough_1              int64         \n",
      " 42  t_recov_severity__q_recov_cough_2              int64         \n",
      " 43  t_recov_severity__q_recov_cough_3              int64         \n",
      " 44  t_recov_severity__q_recov_cough_skipped        int64         \n",
      " 45  body_temp_loc_1                                int64         \n",
      " 46  body_temp_loc_2                                int64         \n",
      " 47  body_temp_loc_3                                int64         \n",
      " 48  body_temp_loc_4                                int64         \n",
      " 49  body_temp_loc_5                                int64         \n",
      " 50  body_temp_loc_skipped                          int64         \n",
      " 51  symptom_severity__q_congestion_0               int64         \n",
      " 52  symptom_severity__q_congestion_1               int64         \n",
      " 53  symptom_severity__q_congestion_2               int64         \n",
      " 54  symptom_severity__q_congestion_3               int64         \n",
      " 55  symptom_severity__q_congestion_skipped         int64         \n",
      " 56  symptom_severity__cough_q_0                    int64         \n",
      " 57  symptom_severity__cough_q_1                    int64         \n",
      " 58  symptom_severity__cough_q_2                    int64         \n",
      " 59  symptom_severity__cough_q_3                    int64         \n",
      " 60  symptom_severity__cough_q_skipped              int64         \n",
      " 61  t_recov_severity__q_recov_sore_throat_0        int64         \n",
      " 62  t_recov_severity__q_recov_sore_throat_1        int64         \n",
      " 63  t_recov_severity__q_recov_sore_throat_2        int64         \n",
      " 64  t_recov_severity__q_recov_sore_throat_3        int64         \n",
      " 65  t_recov_severity__q_recov_sore_throat_skipped  int64         \n",
      " 66  t_recov_severity__q_recov_headache_0           int64         \n",
      " 67  t_recov_severity__q_recov_headache_1           int64         \n",
      " 68  t_recov_severity__q_recov_headache_2           int64         \n",
      " 69  t_recov_severity__q_recov_headache_3           int64         \n",
      " 70  t_recov_severity__q_recov_headache_skipped     int64         \n",
      " 71  symptom_severity__q_sore_throat_0              int64         \n",
      " 72  symptom_severity__q_sore_throat_1              int64         \n",
      " 73  symptom_severity__q_sore_throat_2              int64         \n",
      " 74  symptom_severity__q_sore_throat_3              int64         \n",
      " 75  symptom_severity__q_sore_throat_skipped        int64         \n",
      " 76  t_recov_severity__q_recov_congestion_0         int64         \n",
      " 77  t_recov_severity__q_recov_congestion_1         int64         \n",
      " 78  t_recov_severity__q_recov_congestion_2         int64         \n",
      " 79  t_recov_severity__q_recov_congestion_3         int64         \n",
      " 80  t_recov_severity__q_recov_congestion_skipped   int64         \n",
      " 81  med_rx_name__2_0                               int64         \n",
      " 82  med_rx_name__2_1                               int64         \n",
      " 83  med_rx_name__2_skipped                         int64         \n",
      " 84  med_otc_name__6_0                              int64         \n",
      " 85  med_otc_name__6_1                              int64         \n",
      " 86  med_otc_name__6_skipped                        int64         \n",
      " 87  body_temp_avail_yn_0                           int64         \n",
      " 88  body_temp_avail_yn_1                           int64         \n",
      " 89  body_temp_avail_yn_skipped                     int64         \n",
      " 90  med_rx_name__5_0                               int64         \n",
      " 91  med_rx_name__5_1                               int64         \n",
      " 92  med_rx_name__5_skipped                         int64         \n",
      " 93  med_otc_yn_0                                   int64         \n",
      " 94  med_otc_yn_1                                   int64         \n",
      " 95  med_otc_yn_2                                   int64         \n",
      " 96  med_rx_name__3_0                               int64         \n",
      " 97  med_rx_name__3_skipped                         int64         \n",
      " 98  t_recov_severity__q_recov_sneezing_0           int64         \n",
      " 99  t_recov_severity__q_recov_sneezing_1           int64         \n",
      " 100 t_recov_severity__q_recov_sneezing_2           int64         \n",
      " 101 t_recov_severity__q_recov_sneezing_3           int64         \n",
      " 102 t_recov_severity__q_recov_sneezing_skipped     int64         \n",
      " 103 med_rx_name__1_0                               int64         \n",
      " 104 med_rx_name__1_1                               int64         \n",
      " 105 med_rx_name__1_skipped                         int64         \n",
      " 106 symptom_severity__q_headache_0                 int64         \n",
      " 107 symptom_severity__q_headache_1                 int64         \n",
      " 108 symptom_severity__q_headache_2                 int64         \n",
      " 109 symptom_severity__q_headache_3                 int64         \n",
      " 110 symptom_severity__q_headache_skipped           int64         \n",
      " 111 med_otc_name__3_0                              int64         \n",
      " 112 med_otc_name__3_1                              int64         \n",
      " 113 med_otc_name__3_skipped                        int64         \n",
      " 114 t_experience__q_pain_0                         int64         \n",
      " 115 t_experience__q_pain_1                         int64         \n",
      " 116 t_experience__q_pain_2                         int64         \n",
      " 117 t_experience__q_pain_3                         int64         \n",
      " 118 t_experience__q_gad_mdd_0                      int64         \n",
      " 119 t_experience__q_gad_mdd_1                      int64         \n",
      " 120 t_experience__q_gad_mdd_2                      int64         \n",
      " 121 t_experience__q_gad_mdd_3                      int64         \n",
      " 122 t_recov_severity__q_recov_headaches_0          int64         \n",
      " 123 t_recov_severity__q_recov_headaches_1          int64         \n",
      " 124 t_recov_severity__q_recov_headaches_2          int64         \n",
      " 125 t_recov_severity__q_recov_headaches_3          int64         \n",
      " 126 t_recov_severity__q_recov_headaches_skipped    int64         \n",
      " 127 symptom_severity__fever_q_0                    int64         \n",
      " 128 symptom_severity__fever_q_1                    int64         \n",
      " 129 symptom_severity__fever_q_2                    int64         \n",
      " 130 symptom_severity__fever_q_3                    int64         \n",
      " 131 symptom_severity__fever_q_skipped              int64         \n",
      " 132 t_recov_severity__q_recov_aches_0              int64         \n",
      " 133 t_recov_severity__q_recov_aches_1              int64         \n",
      " 134 t_recov_severity__q_recov_aches_2              int64         \n",
      " 135 t_recov_severity__q_recov_aches_3              int64         \n",
      " 136 t_recov_severity__q_recov_aches_skipped        int64         \n",
      " 137 severity_overall_1                             int64         \n",
      " 138 severity_overall_2                             int64         \n",
      " 139 severity_overall_3                             int64         \n",
      " 140 med_otc_name__2_0                              int64         \n",
      " 141 med_otc_name__2_1                              int64         \n",
      " 142 med_otc_name__2_skipped                        int64         \n",
      " 143 t_difficulty__q_selfcare_0                     int64         \n",
      " 144 t_difficulty__q_selfcare_1                     int64         \n",
      " 145 t_difficulty__q_selfcare_2                     int64         \n",
      " 146 symptom_severity__chills_q_0                   int64         \n",
      " 147 symptom_severity__chills_q_1                   int64         \n",
      " 148 symptom_severity__chills_q_2                   int64         \n",
      " 149 symptom_severity__chills_q_3                   int64         \n",
      " 150 symptom_severity__chills_q_skipped             int64         \n",
      " 151 recovery_48_h_1                                int64         \n",
      " 152 recovery_48_h_2                                int64         \n",
      " 153 recovery_48_h_3                                int64         \n",
      " 154 med_rx_name__6_0                               int64         \n",
      " 155 med_rx_name__6_1                               int64         \n",
      " 156 med_rx_name__6_skipped                         int64         \n",
      " 157 med_rx_yn_0                                    int64         \n",
      " 158 med_rx_yn_1                                    int64         \n",
      " 159 med_rx_yn_2                                    int64         \n",
      " 160 t_recov_severity__q_recov_chills_0             int64         \n",
      " 161 t_recov_severity__q_recov_chills_1             int64         \n",
      " 162 t_recov_severity__q_recov_chills_2             int64         \n",
      " 163 t_recov_severity__q_recov_chills_3             int64         \n",
      " 164 t_recov_severity__q_recov_chills_skipped       int64         \n",
      " 165 med_rx_name__7_0                               int64         \n",
      " 166 med_rx_name__7_1                               int64         \n",
      " 167 med_rx_name__7_skipped                         int64         \n",
      " 168 t_difficulty__q_mobility_0                     int64         \n",
      " 169 t_difficulty__q_mobility_1                     int64         \n",
      " 170 t_difficulty__q_mobility_2                     int64         \n",
      " 171 symptom_severity__sweats_q_0                   int64         \n",
      " 172 symptom_severity__sweats_q_1                   int64         \n",
      " 173 symptom_severity__sweats_q_2                   int64         \n",
      " 174 symptom_severity__sweats_q_3                   int64         \n",
      " 175 symptom_severity__sweats_q_skipped             int64         \n",
      " 176 t_recov_severity__q_recov_sweats_0             int64         \n",
      " 177 t_recov_severity__q_recov_sweats_1             int64         \n",
      " 178 t_recov_severity__q_recov_sweats_2             int64         \n",
      " 179 t_recov_severity__q_recov_sweats_3             int64         \n",
      " 180 t_recov_severity__q_recov_sweats_skipped       int64         \n",
      " 181 symptom_severity__q_sneezing_0                 int64         \n",
      " 182 symptom_severity__q_sneezing_1                 int64         \n",
      " 183 symptom_severity__q_sneezing_2                 int64         \n",
      " 184 symptom_severity__q_sneezing_3                 int64         \n",
      " 185 symptom_severity__q_sneezing_skipped           int64         \n",
      " 186 trigger_datetime                               datetime64[ns]\n",
      " 187 missing_day                                    int64         \n",
      "dtypes: datetime64[ns](2), int64(185), object(1)\n",
      "memory usage: 761.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Strings in temp data, drop for now. Fill nan in recovered_yn and first_report_yn with zeros.\n",
    "df_survey_model = df_survey.drop(columns = ['Unnamed: 0', 'occurrence', 'recovery_datetime', 'first_sx_datetime','body_temp_f'])\n",
    "df_survey_model.fillna(value = {'recovered_yn': 0, 'first_report_yn': 0} ,inplace = True)\n",
    "df_survey_model = df_survey_model.astype({'recovered_yn': 'int64','first_report_yn': 'int64'})\n",
    "df_survey_model['missing_day'] = 0\n",
    "df_survey_model.info(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAs9ElEQVR4nO3deXiU1dn48e89kw0CSQhJgGyEJez7Jqggilbcccfd/rSotdVutvbtbvWt2rdqtdWKthXUirS4oHVDZFNBDGFfAmELgZCELBCyTTJzfn/MEwwQkoEszyz357rmmsmZ80zuTJJzz9meR4wxKKWUCj0OuwNQSillD00ASikVojQBKKVUiNIEoJRSIUoTgFJKhagwuwNoSUJCgsnIyLA7DKWUCihr1qw5ZIxJbK6O3yeAjIwMsrKy7A5DKaUCiojsbamODgEppVSI0gSglFIhShOAUkqFKE0ASikVojQBKKVUiNIEoJRSIUoTgFJKhShNAOo4xhjmf72PkqO1doeilGpnmgDUcd7bUMBPF2zguc9y7Q5FKdXONAEEsYOHa8gvq/K5fk2dmyc+3AbAW9n51NS52ys0pZQf0AQQxL7/RjZ3/GM1LV31rbK2nptfWsWlf17B/vJqvn9Bf47U1PPBxoJTHrOvtApXvaetQ1ZKdSBNAEGqtNJF1t4ydhZXsqXgCB6PweNpOhF8suUgX+4soWdsFN+d2o8fXTSAPgnRzF2596RjiipquP9f2Ux+cgl3/nM11S7tJSgVqDQBBKnl24tp+OD//oYCvjM3i7Mf/4z/rMk/qVFfuO4AKXGdeO2us/jp9EGICPed1491+8r5y5Lj5wL+979bWbSlkGtGp7ByVwmzXs1qdqiops5NblFFs7EeKK/mh2+uY2lOUYu9FbttLThCaaXL7jCUahOaAILUZ9uKSOgSweTMBP6+YjeLtxUR5hR+8u/1/Pjf648N35RVulix4xCXj+yFwyHHjr9+XCrXjE7h6U+3s3JnybG6H2w6yM0T0nnqxlE8ee0IPs89xD2vrmkyCXy2rZALn1rGhU8t5+UVu04Z67yv9/H22v3c+c+vGf/Yp9zwt5Vs2n+4jd+R1qusrefaF77kwXlr7Q5FqTahCSAI1bs9LNtezHkDkrhiRDIut4cpAxJZ/tD5/ORbA3h77X5+8Ka3Efto80HqPYYrRiQf9xoiwmNXDycmKpwF2fkALMjOx1XvYeaENACuH5fG49cMZ9n2Yh55f8txxx86Wsu9r2bTOcLJ+QMTefS/W7nl5VX8+dMdJ80dLNpSyJj0OJ64djgXDu7B3tJKbnhxJUtyik762T7ZfJC3rHg62qdbC6lyuVmx4xCrd5faEoNSbcnvrwegTt/qPaUcrq7j/EGJXDAoie2FFcya0heHQ/jeBZm4PfD0p9tZvbuUv3++m8ykLgxNjjnpdTpFOJmcmcDSnGI8HsMbq/MYkx7HoJ7f1L1xfDq5RUd5acVuZoxKYUKfeADe+CoPl9vDC7eOpXd8Z/60aDtLc4p5+tPtlFTW8shVwwDIL6tia8ER/ufSQdw4Pp0bx6dTdKSGO/75NT98cx1LfjyVbtERgHePwu/e28LBIzVkJETz0vJdlFS6+Med4+kS2fo/5S9yD7FsezH1bsO3z8kgLb7zcc+/u+4AvWKjqPcYHv9wK49cNYzBvWJwNuo5najwSA1R4U5iO4W3Oj6l2pr2AILQ61/lEdspnGmDetA5IoxfXj6EpJioY8/fPbkP8dERzHo1i9yio8fG/Zty/sAkDh2t5S9LctlZXMmtE3ufVOeHFw0gJa4TD7+1gWqXmzq3h9e+2svkzAT6JXYhzOngZ9MH8eGDk5k1pS9zV+5lwRrvp/jFW72f8i8c3OPY6yXFRPH0jSOpqKnnT4tyjpVvLahgf3k1HmOY+eIqPtx0kKw9pdz76pqTehVVLu/KpnGPLmLKk0t4fmkuh6vrTvme5RYd5ZaXv+KVL/fw2qq9TPvTMp5bvOPYfElppYvl24u5clQyP75oANl55Vz+3OdMf2Y5y7cX4z5hXqXkaC3ff2MtZz/+GZc/t4ID5dWn/N5K2UUTQJApqqjh400HuX5sKp0inE3WiY4M4+7JfSivquOc/t25cHDSKV9vygDvFeWe/nQ7afGduGJk8kl1OkeE8cS1I9h9qJJfvL2RZxfvoPBILd8+J+Okuj+bPogRqbE8vzQXYwyfbDlI38Ro+iZ2Oa7eoJ4x3DaxN69/lUdu0VHAOwQjAo9fM5x6j4eHLh7IE9Y8xNyVe447fmlOMV/uLGFi3+6kxHXiyY9yOPeJz5jz5fH1Gvz9811EhjlY+fAFLPvpVL41tAd/WrSdu+dm4fYYFq7bT73HcOXIZGZOSGfZQ1N58roR1NS7uf0fqxn1yCfMz9oHgNtjeGDeWj7efJBbzkqnvLKOm15aRXGF7q5W/kWHgILEkZo6/vRxDntKqqj3GG5p4pN6Y3dMyqC4opY7JmWc8tM/QGLXSIanxLJx/2HuO68/4c6mPzOcm5nAg9MyeebTHQBcMTKZqQNOTixOh3Dj+DR+8fYm3l13gC9yS/jBhZlNvua95/XjlS/38OnWQvondWHRlkJGpcVx4/h0Lhnei5go77DKgux8Xl6xm9sm9SYyzJv0Pt1aSGyncJ65cRRhTgdbDhzh9+9v4XfvbWb6sJ70aNQjOnS0lgXZ+7l2TCrdu0QC8NxNoxmZGsdjH2zl31n7eGnFbsakxzGkl3f4q3f3aHp3j+bKkcl8vPkgL63YxZMfbWPGqBReWLqTL3JLeOLa4dw4Pp0Zo1O4afYqvvevbOb8vwkUV9QeN7xU7XIz7+s8Ptp0kIuH9uSWienHfg6l2pP2AILE++sLmLNyL5/nHuLioT3okxDdbP3oyDB+c8VQMlqoB3Dd2FSGJsdw7diUZus9cEEm903tx1M3jOTZmaOOW1XU2GXDexHuFH62YAORYQ5uO0Wy6hkbxcAeXVmxo5iCw9Vs3H+Yi4Z4h4oaGn+A707tz8EjNbyzdj/gnQRfsq2ICwYlEWYlrCHJMTx29TA8hmP1wDv0c//r2bjqPdx1bp9j5SLCXef2YXhKLL9+dzP7y6u5//z+JyXLqHAnV41K4SffGsihoy6e/nQ7z362gxmjkrlhnHeyfEx6Nx6/djhf7S5lxG8/YfKTS1hqTXDX1LmZOXslv3tvC/ll1Tzy/hZuffmrU+7ZUKotaQIIEsu2F5ES14ntj17C324d26avfcfZGfz3gcktfip1OISfTR/ENWNSm+1VxHWO4PyBSdTWe7h+3DefupsyZUACX+8u48Vlu3CIN3mcaHJmAsNTYnlpxW6MMWTnlVNWVXfcvAJA38QujEmPY0F2PsYYSitdzPjrF2wpOMLj1wynf9Lxw1AOh/DQxQNxuT0M6tmVCwY1M1SWmUh6fGdeWLqTuE7h/PbKoce9B1ePTuXnlwzi2rEpJMdG8eziHRhj+NU7m1iff5jnbhrN5z87n99fNZSv95TxdqMk1aDa5eaHb65ja8GRU8ah1OnQBBAE6twevsgtYcqARJwOabbx9Re3TuxNTFQY35nct9l6UwYk4nJ7eOXLPVwyvBe9u5/cYxERbjnLuxpp3b5yPt58kHCnMGVAwkl1rx2byvbCo2zaf4QFa/I5WlvP/HsmMXNCepPff3JmAj/51gAenTGs2ffV4ZBjPZlfXzGEuM4RJ9W557x+/OGaEdw3tR/ZeeXc9vfV/HtNPg9c0J8rRiZbP0dvRqbF8cRH26ioOX7S+uPNB3l77X5+tmDDSZPOSp0JTQBBIHtvGUdr65k6MNHuUHw2ZUAiG357cZMNemPjM+KJCvf+md53Xr9T1rt0RC8iwxy8uGwX//oqj4uH9qRr1MlLLy8fnkyXyDB+//4W3lidx9je3Rjc6+QlsA1EvEtnx2XEt/gz3XlOBm/OmsiVTUyUN3b9uDQSu0byxc5DPDAtkx9cOODYcw6H8JsrhnDoaC1X/fWL4z7tv7V2PxFhDjbkH2be13ktxqNUSzQBBIFl24sJcwhn9+tudyhtLircyRUjkrlyZDLDUmJPWS8mKpyLh/a0NrZ5Vwg1JbZzOL+7ciir95Sy61AlN53ik/+ZCHc6OKtv9xZ7YFHhTv5553gW3Hc2P7powElzJWPSu/Ha3WdxtKaeO/6xmpo6N0VHavh8RzHfmdyHSX27838f53C0tr7NYlehSRNAgKt3e3h/QwHjMro1+Yk3GPzx+pE8e9PoFutdOzYVgFvO6t1sz+KaMSlcPTqFHjGRTc4pdIRhKbGMSe92yufP7pfAMzNHUVRRy/ysfSzI3o/HwNWjU/jp9IGUVdXx2qq9HRixCka6DDTAvbV2P3mlVfzq8iF2h2K7KZkJ/HnmqJMmf08kIjx1w0iq69yn3CvhDyb17c643t14atF2KmrqOad/d/ondQW8Q2gvLd/F7ZN60zlC/43VmdEeQACrc3t47rMdDE+JbXYzV6gQEa4alUK0D6eFEBG/bzhFhAemZVJeVceEjHhevG3csecenJZJSaWLpxdttzFCFej8+z9ANWvBmnz2lVbzyJ3Nr1BRgWvKgEQWfu8cBvbsetwy3LG9u3HbxN68tGI3o9K6cdkIe4ayVGDTHkCActV7eO6zXEalxQXU6h91+kakxjW5B+NXlw9hdHocv3xnoy4LVWdEE0CAmp+1j/3l1fzwogH66T9ERYQ5uH1Sb8qq6nRzmDojmgACUJ3bw/NLchnbuxtTMk/e7KRCx1l9vEt/v9LrE6gzoAkgAL2/4QAHDtdw//n99NN/iEuO60R6fGdW7SqxOxQVgHxOACLiFJG1IvK+9XW8iCwSkR3WfbdGdX8uIrkikiMiFzcqHysiG63nnhVtvU6bMYYXl+1iQI8uTZ5tU4WeiX3jWb27VE8gp07b6fQAHgS2Nvr6YWCxMSYTWGx9jYgMAWYCQ4HpwPMi0jCD9QIwC8i0btNbFX0IWra9mG0HK5g1pd8pz7apQstZfbpzuLqObQcr7A5FBRifEoCIpAKXAS83Kr4KmGM9ngPMaFQ+zxhTa4zZDeQCE0SkFxBjjFlpjDHA3EbHKB+9uGwXPWOiWjzfjAodE61TgHy585DNkahA42sP4Bngp0Dj6+71MMYUAFj3DeMRKcC+RvXyrbIU6/GJ5ScRkVkikiUiWcXFxT6GGPzW7ytn5a4S7jq3DxFhOn2jvFLiOpGZ1IXPthUdK9tRWMHLK3bp8lDVrBZbERG5HCgyxqzx8TWbGpcwzZSfXGjMbGPMOGPMuMREXePeYPaKXXSNCmPmhDS7Q1F+ZtrgHqzeXcoR6xTSv1m4mUf/u5UfvrmOerenhaNVqPLlY+Q5wJUisgeYB1wgIq8BhdawDtZ9w8ePfKBxC5UKHLDKU5soVz6od3tYuq2IK0cmB+1J39SZu3BwEvUew7KcYnKLKvhyZwkjU2NZuP4A976WTcHhamYv38mOQp0nUN9oMQEYY35ujEk1xmTgndz9zBhzK7AQuMOqdgfwrvV4ITBTRCJFpA/eyd7V1jBRhYhMtFb/3N7oGNWCnMIKKl1uJvRp+bz0KvSMTu9GfHQEn2wpZM6Xe4lwOvjHneN55KqhLN5WyKQ/fMb/frCNe15bQ02d2+5wlZ9ozbmAHgfmi8hdQB5wPYAxZrOIzAe2APXA/caYhr+4+4BXgE7Ah9ZN+WDN3jLAew4YpU7kdAjnD0xiQbZ3mu3q0Sl07xLJ7ZMySInrxKIthQxNieVX72ziz4t38LPpg2yOWPmD00oAxpilwFLrcQkw7RT1HgMea6I8Cxh2ukEqyNpTRs+YKFLiOtkdivJTP7tkIKPSYimvquOasd+Mtk4b3INp1imyN+aXM3v5Lr59dgZJMVF2har8hC4lCRBr9pYxtnc33fmrTimpaxS3Tcrg+9MyT/lB4Z7z+uH2GBau1+k3pQkgIBw8XMP+8mod/lGt1i+xCyNTY3kre7/doSg/oAnAz+0sPspD/1kPoBPAqk3MGJ3CloIj5OjO4ZCnCcCPHa6q44a/rWTdvnJ+ffmQZi+KrpSvrhiZjEPgw00FdoeibKZXBPNjzyzeTlmVi/e+fy5Dk7XxV20joUskXSLDKK+qszsUZTPtAfip3KKjzF25l5kT0rXxV20uIsxJbb3uEA51mgD81ML1BzDG8KOLBtgdigpCkWEOXJoAQp4mAD+1amcJw1JiSegSaXcoKgiFO4U6PUdQyNME4IeqXW7W7StnUt/udoeiglSE9gAUmgD8UnZeGS6359h53pVqaxFhDlzaAwh5mgD80MqdJTgdwvgMXfev2keEU3sAShOAX1q5q4ThKbF0idRVuqp96BCQAk0AfufQ0VrW5pUxJTPB7lBUEIsIc1KrQ0AhTxOAn/lo00E8Bi4Z3svuUFQQ0yEgBZoA/M4HGwvomxDNoJ5d7Q5FBTHvPgC9MEyo0wTgRw4drWXVrhIuHd5LT/us2pWuAlKgCcBv7C2p5MF5a/EYuFSHf1Q7C3eKDgEpPRmcP/B4DDNnr6Kipp5HZwxjSHKM3SGpIBcR5qDObewOQ9lME4Af2HTgMAWHa3j6xpFcPTq15QOUaqUIp1N7AEqHgPzBih2HAJicmWhzJCpU6D4ABZoA/MKy7cUMTY7RE7+pDtMwCWyMDgOFMk0ANquoqSN7bxlTBuinf9VxIsO8//q6Eii0aQKw2Re5JdR7DJN156/qQBFOKwHoMFBI0wRgozq3h2c+3U7PmCjG9dYTv6mOExGmCUDpKiBb1NS52VF4lA83FbDtYAV/u3XssX9IpTpChA4BKTQBdChXvYdfvrOR99YXUF3n3YZ/4eAeXDy0h82RqVATrkNACk0AHcYYw2/f28z8rHxmjk/jvAGJxHQKZ3xGvJ72QXW4hh6AXhYytGkC6CDvrjvAv77K497z+vHwJYPsDkeFuIZJ4FrtAYQ0HXjuIP/8YjcDe3TloYsH2h2KUt8sA9UEENI0AXSAnIMVrM8/zI3j03A6dLhH2U9XASnQBNAh/p21j3CnMGN0it2hKAXoKiDlpQmgnRVV1LAgO58LB/cgPjrC7nCUAnQjmPLSBNCOqlz13D0ni5o6D9+7oL/d4Sh1jA4BKdBVQO3qL5/lsnH/YV66bRxDk2PtDkepY3QISIEPPQARiRKR1SKyXkQ2i8jvrPJ4EVkkIjus+26Njvm5iOSKSI6IXNyofKyIbLSee1aCeAG8x2N4Z+1+zh+YxIVDdKOX8i+6DFSBb0NAtcAFxpiRwChguohMBB4GFhtjMoHF1teIyBBgJjAUmA48LyJO67VeAGYBmdZtetv9KP4la28ZBw7XcOXIZLtDUeokOgSkwIcEYLyOWl+GWzcDXAXMscrnADOsx1cB84wxtcaY3UAuMEFEegExxpiVxnsS8rmNjgk6C9fvJyrcwUX66V/5oYYegO4EDm0+TQKLiFNE1gFFwCJjzFdAD2NMAYB1n2RVTwH2NTo83ypLsR6fWN7U95slIlkiklVcXHwaP45/qHN7+GDjQaYN7kF0pE6zKP+jPQAFPiYAY4zbGDMKSMX7aX5YM9WbGtc3zZQ39f1mG2PGGWPGJSYG3oVSlmwrorTSxdWjdN2/8k+aABSc5jJQY0w5sBTv2H2hNayDdV9kVcsH0hodlgocsMpTmygPOvOz8knsGsnUgYGXvFRoCHMIIroKKNT5sgooUUTirMedgAuBbcBC4A6r2h3Au9bjhcBMEYkUkT54J3tXW8NEFSIy0Vr9c3ujY4JGUUUNS3KKuGZMCmFO3Wah/JOIEOHUC8OHOl8GqHsBc6yVPA5gvjHmfRFZCcwXkbuAPOB6AGPMZhGZD2wB6oH7jTFu67XuA14BOgEfWreg8lb2ftwew/Vj01qurJSNIsIcugw0xLWYAIwxG4DRTZSXANNOccxjwGNNlGcBzc0fBLTaejf//GI3E/vG0z+pi93hKNWsyDCHDgGFOB2jaENvZ++n8Egt352qp31Q/i9ch4BCniaANuLxGF5cvovhKbFMzkywOxylWhQRpgkg1GkCaCPr88vZfaiSb5+ToZd4VAEhwunQjWAhThNAG1mSU4xD4IJBSS1XVsoPaA9AaQJoI0u2FTEmvRtxnfWc/yowROgkcMjTBNAGio7UsHH/Yc7XT/8qgEQ4dRloqNME0AaWbveer0h3/qpAokNAShNAG1izp4z46AiG9IqxOxSlfBapCSDkaQJoA3mlVWR076yrf1RA0TkApQmgDeSVVpEe39nuMJQ6LboRTGkCaKU6t4eCw9WkaQJQAUZPBqc0AbTSgfJqPAZNACrg6BCQ0gTQSvtKqwF0CEgFnIgwB3XaAwhpmgBaKa+0CtAegAo8EWEOarUHENI0AbTSvrIqwp1Cz5gou0NR6rREWnMAxjR5ZVYVAjQBtFJeaRUpcZ1wOnQJqAosDdcFrnNrAghVmgBaKb+0Sod/VEBqSACLthTi8WgSCEWaAFopTxOAClBn90sgJa4T9/8rm0fe32J3OMoGmgBa4WhtPWVVdaR10wSgAs+wlFiWPTSVG8el8eqqvewqPmp3SKqDaQJoheKKWgB6xkbaHIlSZybM6eAnFw8kMszB/32SY3c4qoNpAmiF0koXAN30GgAqgCV2jeTuyX35YONBthdW2B2O6kCaAFqhzEoA8dGaAFRgu/PsDCLCHLy6cq/doagOpAmgFUqrtAeggkN8dASXj+jFW9n5HK2ttzsc1UE0AbSC9gBUMLl9UgaVLjdvZ+fbHYrqIJoAWqG0ykVEmIPOEU67Q1Gq1UamxpIcG8WavWV2h6I6iCaAViirdBHfOUIvBKOCgoiQ2DWS0qo6u0NRHUQTQCuUVtYR1znc7jCUajPx0RGUVtbaHYbqIJoAWqGsyqXj/yqoxEdHUlapPYBQoQmgFcoqXXTTBKCCSHx0OCXaAwgZmgBaoazKOwegVLCIj46kps5DlUuXgoYCTQBnyO0xlFfXaQ9ABZX4aO+cVsMudxXcNAGcocPVdRgD8ToJrIJIfLT3vFaaAEKDJoAzdOw8QNoDUEGkYVFDiSaAkNBiAhCRNBFZIiJbRWSziDxolceLyCIR2WHdd2t0zM9FJFdEckTk4kblY0Vko/XcsxLAC+jLqnQXsAo+DX/PZZoAQoIvPYB64MfGmMHAROB+ERkCPAwsNsZkAoutr7GemwkMBaYDz4tIw1bZF4BZQKZ1m96GP0uH0jOBqmDUkAB0CCg0tJgAjDEFxphs63EFsBVIAa4C5ljV5gAzrMdXAfOMMbXGmN1ALjBBRHoBMcaYlcZ7Feq5jY4JOHoeIBWMYqLCCHOIJoAQcVpzACKSAYwGvgJ6GGMKwJskgCSrWgqwr9Fh+VZZivX4xPKApGcCVcFIROgWHaEJIET4nABEpAuwAPiBMeZIc1WbKDPNlDf1vWaJSJaIZBUXF/saYocqq3TRKdxJJz0RnAoy3aMjdBI4RPiUAEQkHG/j/7ox5i2ruNAa1sG6L7LK84G0RoenAges8tQmyk9ijJltjBlnjBmXmJjo68/SYYwxrNpVSu/uei1gFXy6dY7QSeAQ4csqIAH+Dmw1xjzV6KmFwB3W4zuAdxuVzxSRSBHpg3eyd7U1TFQhIhOt17y90TEB5fPcQ2zcf5g7z86wOxSl2lx8Fx0CChVhPtQ5B7gN2Cgi66yy/wEeB+aLyF1AHnA9gDFms4jMB7bgXUF0vzHGbR13H/AK0An40LoFnL8uyaVHTCRXjwnYKQylTkmHgEJHiwnAGPM5TY/fA0w7xTGPAY81UZ4FDDudAP1NblEFq3aV8j+XDiIyTMf/VfDp1jmCw9V11Ls9hDl1r2gw09/uaVqyzTspfdmIZJsjUap9dO9i7QWo0l5AsNMEcJqWbi9iQI8upMR1sjsUpdpFcqz3b3tfabXNkaj2pgngNFTW1rN6dylTBya1XFmpANU/qQsAO4uP2hyJam+aAE7DF7mHqHMbpg7wv6WpSrWV1G6diHA6NAGEAE0Ap2H5jmKiI5yMy4i3OxSl2k2Y00FGQmd2FlXaHYpqZ5oATkPWnjLGZcQTEaZvmwpu/RK7sEt7AEFPWzIfHampI6ewgrG9u7VcWakA1z+pC3tLq3DVe+wORbUjTQA+WpdXjjEwJl0TgAp+/RK74PYY9pboMFAw0wTgozV7y3AIjEyLtTsUpdpdv0RdCRQKNAH4KDuvjIE9Y+gapdcAVsGvb2I0ADuLtQcQzDQB+MDtMazNK2ds7zi7Q1GqQ0RHhpEcG0XOwQq7Q1HtSBOAD/LLqjhaW8+IlDi7Q1Gqw4xIjWPdvnK7w1DtSBOAD6pc3pOZdo3y5eSpSgWHMb3jyCut4tDRWrtDUe1EE4APGpbC6fp/FUoaVrxl7y2zORLVXrRF84HLrQlAhZ5hKbGEO4XsvHK7Q1HtRFs0HxzrAei50VUIiQp3MjQ5luw87QEEK23RfKBDQCpUjUnvxob8curcuiM4GGmL5oNaTQAqRE3oE09NnYcVO4rtDkW1A23RfNAwBxCpCUCFmGmDk+gZE8XfP99tdyiqHWiL5oNv5gD0GsAqtIQ7HdxxdgZf5Jbw0aYCFm8txBhjd1iqjWgC8EFDAogM17dLhZ6bJ6TTKdzJva9lc9ecLJbkFNkdkmoj2qL5wFXv3Qimq4BUKIrtHM6Lt43lj9eNIKlrJHNX7rU7JNVGdGurD3QSWIW6KdZlUPPLqnn2sx3sOVRJRkK0zVGp1tIWzQe6DFQpr5vPSscpopPCQUJbNB+43B5EIMwhdoeilK16xEQxc0Iar67ay0ebCuwOR7WSJgAfuOo9RDgdiGgCUOqXlw1hVFocP5q/Xk8XHeA0Afigtt6jwz9KWaLCnbx421iiI8P4ztwsyqtcdoekzpC2aj5wuT26CUypRnrERPG3W8dScLian/x7g+4NCFDaqvmgYQhIKfWNsb278dDFA/l0ayGLthTaHY46A9qq+cClQ0BKNenb5/RhYI+u/HbhZqpc9XaHo06Ttmo+0ASgVNPCnQ4evXoYBw7XMHv5LrvDUadJWzUf1Na7iQzT8wAp1ZTxGfFcNrwXLy7bRdGRGrvDUadBE4APXG7tASjVnJ9OH0i9x8MTH+UAUHikhn2lVTZHpVqip4LwgU4CK9W83t2juWdKP/6yJJe0+E68unIv5dV13DAujV9cNpgukdrU+KMWWzUR+YeIFInIpkZl8SKySER2WPfdGj33cxHJFZEcEbm4UflYEdloPfesBNCuKp0DUKplP7gwkwkZ8Tzz6Q7CnMJNE9KYn7WP+1/Ppl6vKOaXfGnVXgGmn1D2MLDYGJMJLLa+RkSGADOBodYxz4tIw+D5C8AsINO6nfiafks3ginVsjCng+duHs3NZ6Xz5qxJPDpjOI/NGMay7cX84cNtdoenmtBiq2aMWQ6UnlB8FTDHejwHmNGofJ4xptYYsxvIBSaISC8gxhiz0nh3jMxtdIzf0zkApXzTIyaK/716+LEzhc6ckM41o1OYtzqPWuu06sp/nGmr1sMYUwBg3SdZ5SnAvkb18q2yFOvxieVNEpFZIpIlIlnFxfZfi9RV7yFS5wCUOiOXDu9FpctN1p4yu0NRJ2jrVq2pcX3TTHmTjDGzjTHjjDHjEhMT2yy4M6VzAEqdubP7dyfC6WDJNr2SmL8501at0BrWwbpv+M3mA2mN6qUCB6zy1CbKA4IOASl15jpHhHFW33i9lKQfOtNWbSFwh/X4DuDdRuUzRSRSRPrgnexdbQ0TVYjIRGv1z+2NjvF7ugxUqdaZOjCJncWVujfAz/iyDPQNYCUwUETyReQu4HHgIhHZAVxkfY0xZjMwH9gCfATcb4xpmPm5D3gZ78TwTuDDNv5Z2o2uAlKqdc6zLim5fIf9c3rqGy3uzjDG3HSKp6adov5jwGNNlGcBw04rOj/g9hjcHqOnglCqFfolRpPUNZJVu0q55azedoejLPqxtgV6PWClWk9EmNSvO6t2lei1A/yItmot0ASgVNuY2Lc7xRW17CyutDsUZdFWrQW1bu8UhiYApVpnUt/uAKzaVWJzJKqBtmotaOgB6EYwpVqnd/fO9IyJYqUmAL+hrVoLdAhIqbYhIpzdrzsrd5bg9ug8gD/QVq0FLrcmAKXaynkDEymtdLFp/2G7Q1FoAmjRsR6ADgEp1WqTMxMRgaU5uh/AH2ir1gIdAlKq7cRHRzAiNY5l2/W0EP5AW7UW1GoCUKpNnTcgkXX7yimvctkdSsjTVq0F2gNQqm1NHZiIx8CiLYV2hxLytFVrQUMPIFITgFJtYnRaHP2TujB35V7dFWwzbdVa0LAKSBOAUm1DRLjz7Aw27j/Mmr16kRg7aavWgm9WAenJ4JRqK9eMSSEmKozZy3dpL8BGmgBaoHMASrW9zhFhzJrSl0+2FPL80p12hxOyWjwddKhz1eu5gJRqD9+d2p+dxZX88eMc+iREc+nwXnaHFHK0VWuB7gRWqn04HMKT141gZGosv3pnE2WVuiy0o2mr1gLdCaxU+wl3Onj82hEcrq7jl+9u0nMEdTBt1VrQkADCnWJzJEoFp8G9YvjhRQP474YCvvv6Gmrq3C0fpNqEJoAWNFwP2Hste6VUe7j//P78+vIhfLy5kH9+scfucEKGJoAW1NZ79FoASnWA/3duH8b27sY7a/fbHUrI0JatBS63RyeAleogM0Ylk1NYwdaCI3aHEhK0ZWuBq96ju4CV6iCXjUgmzCG8s057AR0h5PcBVLnq+e+GAg4erqGk0kVppQu3MZQedZFXWsWRmjq6R0fYHaZSISE+OoLJmQksXHeAn148CKdD597aU8gmgJo6N29l7+eZT7dTVFELQExUGN2iI3A6hJiocMZldKOgvIbRvePsDVapEHLj+DTufS2bJduKuHBID7vDCWohmQByiyqYOfsrDh2tZVRaHH+5eQyj0uJ0rF8pPzBtcA+Sukby+ld7NQG0s5BLAMYYfrtwC3VuD/+6+ywm9euuSzyV8iPhTgczJ6Tz3Gc72FdaRVp8Z7tDCloh95F38dYiPs89xA8uzOTs/gna+Cvlh2aOT8Mhwqur9todSlALqQRgjOGPH+fQNzGaWyf2tjscpdQpJMd14tLhvXjjqzwqaursDidohVQCWLWrlJzCCu49rx/hurlLKb82a3JfKmrreWN1nt2hBK2QagXnrtxDXOdwrhyZbHcoSqkWDE+NZVLf7sxevpu8kiq7wwlKIZMADpRX88mWQm4cn0ZUuF7dS6lA8Jsrh1Dv8XDTS6s0CbSDkEkAf12Si0PgNh37VypgDOoZw2t3nUWlq16TQDsIiQSQV1LFm1/vY+b4dFK76ZIypQLJsJTYY0ngmhe+ZO7KPcdO065aJyQSwP99koPTIXzvgv52h6KUOgPDUmKZN2sifRI68+t3N/PtV1ZT5aq3O6yA1+EJQESmi0iOiOSKyMPt/f3eys5n4foD3HNeP3rERLX3t1NKtZNBPWOYf88k/njdCFbuLOGGF1fy7rr91NbrBWTOVIfuBBYRJ/BX4CIgH/haRBYaY7a09fd65YvdbC2oYOH6A0zoE88D+ulfqYAnIlw/Lo2uUeH8/v0tPDhvHenxnXn4kkFcMChJF3icpo4+FcQEINcYswtAROYBVwFtngD+vSafwiO1jO8Tzx+vG0GYrvtXKmhMH9aTbw3pwbIdxTz6/ha++3o2UeEOErpE4hDB6RBEwCFCIO/1f/+Bc4kMa7+k1tEJIAXY1+jrfOCsEyuJyCxgFkB6evoZfaO3vnt2u75xSil7ORzC+QOTOLd/Ap/vOMTnuYcoq3Lh8Rg8BtzGYExgX2S+vdNXRyeApn6ak35DxpjZwGyAcePGndFvUBt/pUJDuNPB+YOSOH9Qkt2hBJyOHhfJB9IafZ0KHOjgGJRSStHxCeBrIFNE+ohIBDATWNjBMSillKKDh4CMMfUi8j3gY8AJ/MMYs7kjY1BKKeXV4ReEMcZ8AHzQ0d9XKaXU8XRtpFJKhShNAEopFaI0ASilVIjSBKCUUiFK/H2nnIgUA/5+ZegE4JDdQZyhQIs90OJtoHF3vECMvS1j7m2MSWyugt8ngEAgIlnGmHF2x3EmAi32QIu3gcbd8QIx9o6OWYeAlFIqRGkCUEqpEKUJoG3MtjuAVgi02AMt3gYad8cLxNg7NGadA1BKqRClPQCllApRmgCUUipUGeuqOaFyw3s9giXAVmAz8KBVHg8sAnZY992s8ouANcBG6/6CRq811irPBZ7FGlJr4ns2WQ+YAmQD9cB1ARb7j/BeynMDsBjvmmN/jvdeq3wd8DkwJFDe60bPX4f3AkrjAiFu4E6g2HrP1wF3B9J7DtyA9298M/Avf48ZeLrRe70dKG+xTWmpQrDdgF7AGOtxV+uNGgI8CTxslT8MPGE9Hg0kW4+HAfsbvdZqYBLeK519CFxyiu/ZZD0gAxgBzMW3BOBPsZ8PdLYe3we86efxxjSqcyXwUaC8141iWA6sovkE4Ddx400AfwnQ/81MYC3fNNxJ/h7zCXW+j/d0+82/577+coL1BryLNyvnAL0a/VJzmqgrQAkQadXZ1ui5m4AXT/EH0mw94BV8SAD+GLtVPhr4IoDivQn4MJDea+AZ4HJgKc0kAH+Km9NMAH4W+5O00GPxt5hPqPclcFFL8Yb0HICIZOBtvL4CehhjCgCs+6YuMHotsNYYU4v3Avf5jZ7Lt8pO5Gu9QI79LryfRPw6XhG5X0R24v3nfqC5eP0pdhEZDaQZY973NWZ/iLvhNUVkg4j8R0TS8JEfxD4AGCAiX4jIKhGZHgAxN8TRG+gDfNZSzB1+QRh/ISJdgAXAD4wxR0Saul79cfWHAk8A32ooaqKaaepQH+v5zJ9iF5FbgXHAef4erzHmr8BfReRm4JfAHc0G4gexi4gD79junS3FekIc/vCevwe8YYypFZF7gTnABQESexjeYaCpeK9dvkJEhhljyv045gYzgf8YY9zNBkGIrgISkXC8v6zXjTFvWcWFItLLer4XUNSofirwNnC7MWanVZyP9w+jQSpwQEScIrLOuj1yqnrBELuIXAj8ArjS+hTj1/E2Mg+Y0VS8fhh7V7xjxUtFZA8wEVgoIqc8X4yfxI0xpqTR38VLeCcvm+UvsVvPvWuMqTPG7MY7pJPp5zE3mAm80VSsJznT8blAveHNoHOBZ04o/yPHT9o8aT2OA9YD1zbxWl/j/YdsmIy59BTfs9l6+DgH4E+x4+3q7gQyAyTezEZ1rgCyAuW9PqHOUpqfBPabuLHGwK3HVwOrAuU9B6YDc6zHCcA+oLs/x2w9NxDYwylWEJ30Wr5UCqYbcC7eLtMGvlkydSnQHe9yxh3WfbxV/5dAZaO667BWBOAd+tiEtyH8y6ne9FPVA8bjzeiVeCeDNgdQ7J8ChY1ed6Gfx/tnvMv01uFdtjc0UN7rE+ospfkE4DdxA3+w3vP11ns+KFDec7yN61N4l4FuBGb6e8zWc78FHve1PdRTQSilVIgKyTkApZRSmgCUUipkaQJQSqkQpQlAKaVClCYApZQKUZoAlFIqRGkCUEqpEPX/AdAu/08OlieQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# No missingness in features if survey taken on a given day\n",
    "# Number of survey datapoints per day (5.2k participants in total)\n",
    "df_survey_model.timestamp.dt.date.value_counts().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill/mark missing survey days in between last and first day\n",
    "missing_survey_rows = []\n",
    "\n",
    "for participant in df_survey_model.participant_id.unique():\n",
    "    date_vals = df_survey_model.loc[df_survey_model.participant_id == participant].timestamp.dt.date.values\n",
    "    missing_dates = list(set(pd.date_range(date_vals.min(),date_vals.max()).date) - set(date_vals))\n",
    "    \n",
    "    for m_date in missing_dates:\n",
    "\n",
    "        row_dict = dict.fromkeys(df_survey_model.columns,0)\n",
    "        info_update = {'participant_id':participant, 'timestamp':m_date, 'trigger_datetime': None, 'missing_day': 1 }\n",
    "        row_dict.update(info_update) \n",
    "        missing_survey_rows.append(row_dict)\n",
    "\n",
    "df_survey_missing = pd.DataFrame(missing_survey_rows)               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill/mark missing activity days in between last and first day\n",
    "\n",
    "df_activity_model = df_daily_activity.drop(columns = ['Unnamed: 0', 'main_start_time'])\n",
    "\n",
    "sleep_features = ['main_in_bed_minutes', 'main_efficiency','nap_count', 'total_asleep_minutes', 'total_in_bed_minutes']\n",
    "step_features = ['activityCalories','caloriesOut', 'caloriesBMR', 'marginalCalories', 'sedentaryMinutes', 'lightlyActiveMinutes', 'fairlyActiveMinutes', 'veryActiveMinutes']\n",
    "\n",
    "df_activity_model['missing_hr'] = 0\n",
    "df_activity_model.loc[df_activity_model['resting_heart_rate'].isnull(), 'missing_hr'] = 1\n",
    "df_activity_model.fillna(value = {'resting_heart_rate': 0}, inplace = True)\n",
    "\n",
    "df_activity_model['missing_sleep'] = 0\n",
    "df_activity_model.loc[df_activity_model['total_asleep_minutes'].isnull(), 'missing_sleep'] = 1\n",
    "\n",
    "for feat in sleep_features:\n",
    "    df_activity_model.fillna(value = {feat: 0}, inplace = True)\n",
    "    \n",
    "#step count derivatives\n",
    "df_activity_model['missing_step'] = 0\n",
    "df_activity_model.loc[df_activity_model['activityCalories'].isnull(), 'missing_step'] = 1\n",
    "\n",
    "for feat in step_features:\n",
    "    df_activity_model.fillna(value = {feat: 0}, inplace = True)\n",
    "\n",
    "df_activity_model['missing_day'] = 0\n",
    "missing_activity_rows = []\n",
    "\n",
    "for participant in df_activity_model.participant_id.unique():\n",
    "    date_vals = df_activity_model.loc[df_activity_model.participant_id == participant].timestamp.values\n",
    "    missing_dates = list(set(pd.date_range(date_vals.min(),date_vals.max()).date) - set(date_vals))\n",
    "    \n",
    "    for m_date in missing_dates:\n",
    "\n",
    "        row_dict = dict.fromkeys(df_activity_model.columns,0)\n",
    "        info_update = {'participant_id':participant, 'timestamp':m_date, 'missing_day': 1 }\n",
    "        row_dict.update(info_update) \n",
    "        missing_activity_rows.append(row_dict)\n",
    "\n",
    "df_activity_missing = pd.DataFrame(missing_activity_rows) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 15\n",
    "total_min_days_with_data = window_size\n",
    "max_missing_days_window = int(window_size * 0.3)\n",
    "\n",
    "\n",
    "train_cut_off_date = datetime.datetime.strptime('2020-2-01', '%Y-%m-%d')\n",
    "valid_cut_off_date = datetime.datetime.strptime('2020-2-10', '%Y-%m-%d')\n",
    "\n",
    "\n",
    "#Append dummy days\n",
    "df_survey_model_ms_filled = df_survey_model.append(df_survey_missing)\n",
    "df_survey_model_ms_filled.sort_values(by = ['timestamp'], inplace = True)\n",
    "\n",
    "df_activity_model_ms_filled = df_activity_model.append(df_activity_missing)\n",
    "df_activity_model_ms_filled.sort_values(by = ['timestamp'], inplace = True)\n",
    "\n",
    "#List to iterate over\n",
    "participant_list = df_survey_model_ms_filled.participant_id.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time = 23:58:29\n",
      "Current Time = 00:07:58\n"
     ]
    }
   ],
   "source": [
    "#Iterate over participants, pick window if missing days no more than win_size/n, link participants data across survey - activity - baseline\n",
    "\n",
    "print(\"Current Time =\", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "train_baseline_index = []\n",
    "train_survey_x = []\n",
    "train_activity_x = []\n",
    "train_y = []\n",
    "\n",
    "\n",
    "validation_date_range = [d.date() for d in pd.date_range(train_cut_off_date, valid_cut_off_date - datetime.timedelta(days = 1))]\n",
    "\n",
    "#Dictionary of lists, a list for every day which will be used to train and validate progressively\n",
    "validation_survey_x = dict.fromkeys(validation_date_range,[])\n",
    "validation_activity_x = dict.fromkeys(validation_date_range,[])\n",
    "validation_baseline_index = dict.fromkeys(validation_date_range,[])\n",
    "validation_y = dict.fromkeys(validation_date_range,[])\n",
    "\n",
    "\n",
    "for participant in participant_list:\n",
    "    df_survey_participant = df_survey_model_ms_filled.loc[df_survey_model_ms_filled.participant_id == participant].reset_index(drop = True)\n",
    "    df_activity_participant = df_activity_model_ms_filled.loc[df_activity_model_ms_filled.participant_id == participant].reset_index(drop = True)\n",
    "    \n",
    "    if len(df_survey_participant) >= total_min_days_with_data and len(df_activity_participant) >= total_min_days_with_data:\n",
    "\n",
    "            existing_dates = df_survey_participant.iloc[window_size-1:].loc[df_survey_participant.missing_day == 0].timestamp.values\n",
    "            \n",
    "            #flu negatives\n",
    "            if df_survey_participant.trigger_datetime.isnull().min():\n",
    "                training_dates = [date for date in existing_dates if date < train_cut_off_date]\n",
    "                validation_dates = [date for date in existing_dates if date >= train_cut_off_date and date < valid_cut_off_date]\n",
    "                \n",
    "                for t_date in training_dates:\n",
    "                    survey_target_index = df_survey_participant.loc[df_survey_participant.timestamp == t_date].index.values[0]\n",
    "                    activity_target = df_activity_participant.loc[df_activity_participant.timestamp == t_date.date()]\n",
    "                    \n",
    "                    #check if day with survey has activity\n",
    "                    if activity_target.empty != True:\n",
    "                        activity_target_index = activity_target.index.values[0]\n",
    "                        \n",
    "                        #check if there are w_size # days of activity leading to the given day\n",
    "                        if len(df_activity_participant.iloc[:activity_target_index+1]) >= window_size:\n",
    "                            \n",
    "                            activity_cropped = df_activity_participant.iloc[activity_target_index - window_size+1:activity_target_index+1]\n",
    "                            survey_cropped = df_survey_participant.iloc[survey_target_index - window_size+1:survey_target_index+1]\n",
    "                            \n",
    "                            if survey_cropped.missing_day.sum() <= max_missing_days_window and activity_cropped.missing_day.sum() <= max_missing_days_window:\n",
    "\n",
    "                                train_baseline_index += [participant]\n",
    "                                train_survey_x.append(survey_cropped.drop(columns = ['participant_id','trigger_datetime','timestamp']).to_numpy())\n",
    "                                train_activity_x.append(activity_cropped.drop(columns = ['participant_id','timestamp']).to_numpy())\n",
    "        \n",
    "                                train_y += [0]\n",
    "\n",
    "\n",
    "                for v_date in validation_dates:\n",
    "                            \n",
    "                    survey_target_index = df_survey_participant.loc[df_survey_participant.timestamp == v_date].index.values[0]\n",
    "                    activity_target = df_activity_participant.loc[df_activity_participant.timestamp == v_date.date()]\n",
    "                    if activity_target.empty != True:\n",
    "                        \n",
    "                        activity_target_index = activity_target.index.values[0] \n",
    "                        \n",
    "                        if len(df_activity_participant.iloc[:activity_target_index+1]) >= window_size:\n",
    "                            \n",
    "                            activity_cropped = df_activity_participant.iloc[activity_target_index - window_size+1:activity_target_index+1]\n",
    "                            survey_cropped = df_survey_participant.iloc[survey_target_index - window_size+1:survey_target_index+1]\n",
    "\n",
    "                            if survey_cropped.missing_day.sum() <= max_missing_days_window and activity_cropped.missing_day.sum() <= max_missing_days_window:\n",
    "\n",
    "                                v_date = v_date.date()\n",
    "                                validation_baseline_index[v_date] = validation_baseline_index[v_date] + [participant]\n",
    "                                validation_survey_x[v_date] = validation_survey_x[v_date] + [survey_cropped.drop(columns = ['participant_id','trigger_datetime','timestamp']).to_numpy()]\n",
    "                                validation_activity_x[v_date] = validation_activity_x[v_date] + [activity_cropped.drop(columns = ['participant_id','timestamp']).to_numpy()]\n",
    "                                validation_y[v_date]  = validation_y[v_date] + [0]                \n",
    "            \n",
    "            #flu positives\n",
    "            else:\n",
    "                trigger_date = df_survey_participant.trigger_datetime.iloc[0]\n",
    "                training_dates = [date for date in existing_dates if date < train_cut_off_date and date <= trigger_date]\n",
    "                validation_dates = [date for date in existing_dates if (date >= train_cut_off_date and date < valid_cut_off_date and date <= trigger_date)]\n",
    "\n",
    "                for t_date in training_dates:\n",
    "                    survey_target_index = df_survey_participant.loc[df_survey_participant.timestamp == t_date].index.values[0]\n",
    "                    activity_target = df_activity_participant.loc[df_activity_participant.timestamp == t_date.date()]\n",
    "                    \n",
    "                    #check if day with survey has activity\n",
    "                    if activity_target.empty != True:\n",
    "                        activity_target_index = activity_target.index.values[0]\n",
    "                        \n",
    "                        #check if there are w_size # days of activity leading to the given day\n",
    "                        if len(df_activity_participant.iloc[:activity_target_index+1]) >= window_size:\n",
    "                            \n",
    "                            activity_cropped = df_activity_participant.iloc[activity_target_index - window_size+1:activity_target_index+1]\n",
    "                            survey_cropped = df_survey_participant.iloc[survey_target_index - window_size+1:survey_target_index+1]\n",
    "                            \n",
    "                            if survey_cropped.missing_day.sum() <= max_missing_days_window and activity_cropped.missing_day.sum() <= max_missing_days_window:\n",
    "\n",
    "                                train_baseline_index += [participant]\n",
    "                                train_survey_x.append(survey_cropped.drop(columns = ['participant_id','trigger_datetime','timestamp']).to_numpy())\n",
    "                                train_activity_x.append(activity_cropped.drop(columns = ['participant_id','timestamp']).to_numpy())\n",
    "\n",
    "                                if (survey_cropped.iloc[-1].trigger_datetime.date() - survey_cropped.iloc[-1].timestamp.date()).days == 0:\n",
    "                                    train_y += [1]\n",
    "                                else:\n",
    "                                    train_y += [0]\n",
    "\n",
    "\n",
    "                for v_date in validation_dates:\n",
    "                    survey_target_index = df_survey_participant.loc[df_survey_participant.timestamp == v_date].index.values[0]\n",
    "                    activity_target = df_activity_participant.loc[df_activity_participant.timestamp == v_date.date()]\n",
    "                    \n",
    "                    if activity_target.empty != True:\n",
    "                        \n",
    "                        activity_target_index = activity_target.index.values[0] \n",
    "                        \n",
    "                        if len(df_activity_participant.iloc[:activity_target_index+1]) >= window_size:\n",
    "                            \n",
    "                            activity_cropped = df_activity_participant.iloc[activity_target_index - window_size+1:activity_target_index+1]\n",
    "                            survey_cropped = df_survey_participant.iloc[survey_target_index - window_size+1:survey_target_index+1]\n",
    "\n",
    "                            if survey_cropped.missing_day.sum() <= max_missing_days_window and activity_cropped.missing_day.sum() <= max_missing_days_window:\n",
    "\n",
    "                                v_date = v_date.date()\n",
    "                                validation_baseline_index[v_date] = validation_baseline_index[v_date] + [participant]\n",
    "                                validation_survey_x[v_date] = validation_survey_x[v_date] + [survey_cropped.drop(columns = ['participant_id','trigger_datetime','timestamp']).to_numpy()]\n",
    "                                validation_activity_x[v_date] = validation_activity_x[v_date] + [activity_cropped.drop(columns = ['participant_id','timestamp']).to_numpy()]\n",
    "\n",
    "                                if (survey_cropped.iloc[-1].trigger_datetime.date() - survey_cropped.iloc[-1].timestamp.date()).days == 0:\n",
    "                                    validation_y[v_date] = validation_y[v_date] + [1]\n",
    "                                else:\n",
    "                                    validation_y[v_date]  = validation_y[v_date] + [0]                    \n",
    "\n",
    "print(\"Current Time =\", datetime.datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative train samples\n",
    "#Take crops of window size number of days which aren't necessarily consecutive\n",
    "window_size = 15\n",
    "train_cut_off_date = datetime.datetime.strptime('2020-2-01', '%Y-%m-%d')\n",
    "valid_cut_off_date = datetime.datetime.strptime('2020-4-01', '%Y-%m-%d')\n",
    "\n",
    "\n",
    "flu_negative_participants = df_survey_model.loc[df_survey_model.trigger_datetime.isnull()].sort_values('timestamp')\n",
    "\n",
    "train_flu_negative_participants = flu_negative_participants.loc[flu_negative_participants.timestamp < train_cut_off_date]\n",
    "\n",
    "neg_train_samples = []\n",
    "\n",
    "for participant in train_flu_negative_participants.participant_id.unique():\n",
    "    train_participant = train_flu_negative_participants.loc[train_flu_negative_participants.participant_id == participant].drop(columns = ['participant_id','trigger_datetime','timestamp'])\n",
    "        \n",
    "    if len(train_participant) >= window_size:\n",
    "        for train_i in range(0,len(train_participant) - window_size + 1):\n",
    "            #if (train_participant.iloc[train_i+window_size-1].timestamp.date() - train_participant.iloc[train_i].timestamp.date()).days == (window_size-1):\n",
    "            neg_train_samples.append(train_participant.iloc[train_i:train_i+window_size].to_numpy())\n",
    "\n",
    "neg_train_y = len(neg_train_samples) * [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Flu positive train samples\n",
    "#Only use up to trigger date. Take crops of window size number of days which aren't necessarily consecutive\n",
    "\n",
    "flu_positive_participants = df_survey_model.loc[df_survey_model.trigger_datetime.notnull()].sort_values('timestamp')\n",
    "train_flu_positive_participants = flu_positive_participants.loc[flu_positive_participants.timestamp < train_cut_off_date]\n",
    "\n",
    "#Only using days up to the trigger date\n",
    "train_flu_positive_participants = train_flu_positive_participants.loc[(train_flu_positive_participants.timestamp.dt.date - train_flu_positive_participants.trigger_datetime.dt.date).dt.days <= 0]\n",
    "\n",
    "pos_train_samples = []\n",
    "pos_train_y = []\n",
    "\n",
    "for participant in train_flu_positive_participants.participant_id.unique():\n",
    "    train_participant = train_flu_positive_participants.loc[train_flu_positive_participants.participant_id == participant]\n",
    "    \n",
    "    \n",
    "    if len(train_participant) >= window_size:\n",
    "        for train_i in range(0,len(train_participant) - window_size + 1):\n",
    "\n",
    "            window_crop = train_participant.iloc[train_i:train_i+window_size]\n",
    "            \n",
    "            #y=1 if window ends with dx\n",
    "            if (window_crop.iloc[window_size-1].trigger_datetime.date() - window_crop.iloc[window_size-1].timestamp.date()).days == 0:\n",
    "                pos_train_samples.append(window_crop.drop(columns = ['participant_id','trigger_datetime','timestamp']).to_numpy())\n",
    "                pos_train_y += [1]\n",
    "                break\n",
    "            else:\n",
    "                pos_train_samples.append(window_crop.drop(columns = ['participant_id','trigger_datetime','timestamp']).to_numpy())\n",
    "                pos_train_y += [0]                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Negative window crops for validation.\n",
    "\n",
    "#Select particip with data before valid_cut_off date\n",
    "valid_flu_negative_participants = flu_negative_participants.loc[(flu_negative_participants.timestamp < valid_cut_off_date)]\n",
    "\n",
    "valid_participant_list = valid_flu_negative_participants.loc[valid_flu_negative_participants.timestamp >= train_cut_off_date].participant_id.unique()\n",
    "\n",
    "\n",
    "#Last day inclusive range\n",
    "valid_date_range = pd.date_range(train_cut_off_date, valid_cut_off_date - datetime.timedelta(days = 1))\n",
    "\n",
    "#List of lists, a list for every day which will be used to train and validate progressively\n",
    "valid_samples = [[] for _ in valid_date_range]\n",
    "valid_y = [[] for _ in valid_date_range]\n",
    "\n",
    "#Iterate over participants\n",
    "for participant in valid_participant_list:\n",
    "    valid_participant = valid_flu_negative_participants.loc[valid_flu_negative_participants.participant_id == participant]\n",
    "    valid_participant.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    #for each participant iterate over date range, find the index of given date and take -window_size\n",
    "    for i in range(len(valid_date_range)):\n",
    "        valid_date = valid_date_range[i]\n",
    "        #find index of row with the given validation date\n",
    "        date_index = valid_participant.index[valid_participant['timestamp'].dt.date == valid_date.date()]\n",
    "        \n",
    "        #if data for given day exists, take all leading days and crop a window\n",
    "        if len(date_index) != 0:\n",
    "            valid_crop = valid_participant.iloc[:date_index[0]+1]\n",
    "            \n",
    "            if len(valid_crop) >= window_size:\n",
    "                valid_samples[i].append(valid_crop.iloc[-window_size:].drop(columns = ['participant_id','trigger_datetime','timestamp']).to_numpy())\n",
    "                valid_y[i].append(0)\n",
    "                \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Positive window crops for validation.\n",
    "\n",
    "#Select particip with data before valid_cut_off date\n",
    "valid_flu_positive_participants = flu_positive_participants.loc[(flu_positive_participants.timestamp < valid_cut_off_date)]\n",
    "\n",
    "valid_participant_list = valid_flu_positive_participants.loc[(valid_flu_positive_participants.timestamp >= train_cut_off_date) & (valid_flu_positive_participants.trigger_datetime >= train_cut_off_date)].participant_id.unique()\n",
    "\n",
    "\n",
    "#Iterate over participants\n",
    "for participant in valid_participant_list:\n",
    "    valid_participant = valid_flu_positive_participants.loc[valid_flu_positive_participants.participant_id == participant]\n",
    "    valid_participant.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    #for each participant iterate over date range, find the index of given date and take -window_size\n",
    "    for i in range(len(valid_date_range)):\n",
    "        valid_date = valid_date_range[i]\n",
    "        date_index = valid_participant.index[valid_participant['timestamp'].dt.date == valid_date.date()]\n",
    "        \n",
    "        #if data for given day exists, take all leading days and crop a window\n",
    "        if len(date_index) != 0:\n",
    "            valid_crop = valid_participant.iloc[:date_index[0]+1]\n",
    "            \n",
    "            if len(valid_crop) >= window_size:\n",
    "                \n",
    "                #y=1 if window ends with dx\n",
    "                if (valid_date.date() - valid_crop.iloc[-1].trigger_datetime.date()).days == 0:\n",
    "                    \n",
    "                    valid_samples[i].append(valid_crop.iloc[-window_size:].drop(columns = ['participant_id','trigger_datetime','timestamp']).to_numpy())\n",
    "                    valid_y[i].append(1)\n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    valid_samples[i].append(valid_crop.iloc[-window_size:].drop(columns = ['participant_id','trigger_datetime','timestamp']).to_numpy())\n",
    "                    valid_y[i].append(0)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert survey data to np arrays. Train valid already split temporally.\n",
    "train_survey_x_stacked = np.stack((train_survey_x))\n",
    "train_activity_x_stacked = np.stack((train_activity_x))\n",
    "\n",
    "train_y  = np.array(train_y)\n",
    "\n",
    "validation_y_list = []\n",
    "validation_survey_x_list = []\n",
    "validation_activity_x_list = []\n",
    "\n",
    "for key in validation_survey_x.keys():\n",
    "    validation_survey_x_list += [np.stack(validation_survey_x[key])]\n",
    "    validation_activity_x_list += [np.stack(validation_activity_x[key])]    \n",
    "    validation_y_list += [np.array(validation_y[key])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#participant ids to indices to time series and baselines\n",
    "train_b_ind = []\n",
    "for p in train_baseline_index:\n",
    "    train_b_ind += [df_baseline.loc[df_baseline.participant_id == p].index.values[0]]\n",
    "    \n",
    "valid_b_ind = []\n",
    "\n",
    "for key in validation_baseline_index.keys():\n",
    "    ind_for_day = []\n",
    "    for p in validation_baseline_index[key]:\n",
    "        ind_for_day += [df_baseline.loc[df_baseline.participant_id == p].index.values[0]]\n",
    "    valid_b_ind += [ind_for_day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline data prepared for modelling\n",
    "baseline_array = df_baseline.drop(['participant_id','Unnamed: 0'],axis=1).to_numpy()\n",
    "\n",
    "baseline_scaled = baseline_array\n",
    "scaler = MinMaxScaler()\n",
    "baseline_scaled = scaler.fit_transform(baseline_array)  \n",
    "\n",
    "pca_var = 0.95\n",
    "pca = PCA(pca_var)\n",
    "pca.fit(baseline_scaled)\n",
    "baseline_pca = pca.transform(baseline_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5229, 136)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_type, n_timesteps, n_features):\n",
    "    if model_type == 'cnn':\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding = 'same' ,input_shape=(n_timesteps,n_features)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        #model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "        model.add(layers.Conv1D(filters=256, kernel_size=3, activation='relu',padding = 'same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "\n",
    "        model.add(layers.Conv1D(filters=128, kernel_size=3, activation='relu',padding = 'same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        #pooling\n",
    "        #dropout\n",
    "        #batch training\n",
    "        model.add(layers.GlobalAveragePooling1D())\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        model.compile( loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',tf.keras.metrics.AUC(),tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    elif model_type == 'lstm':\n",
    "        \n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.LSTM(128, input_shape=(n_timesteps, n_features)))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        model.compile( loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',tf.keras.metrics.AUC(),tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale survey and activity train data\n",
    "n_timesteps_survey = train_survey_x_stacked.shape[1]\n",
    "n_features_survey = train_survey_x_stacked.shape[2]\n",
    "\n",
    "n_timesteps_activity = train_activity_x_stacked.shape[1]\n",
    "n_features_activity = train_activity_x_stacked.shape[2]\n",
    "\n",
    "X_train_survey = train_survey_x_stacked\n",
    "y_train = train_y\n",
    "X_train_scaled_survey = X_train_survey\n",
    "\n",
    "X_train_activity = train_activity_x_stacked\n",
    "X_train_scaled_activity = X_train_activity\n",
    "\n",
    "survey_scalers = {}\n",
    "for i in range(n_features_survey):\n",
    "    survey_scalers[i] = MinMaxScaler()\n",
    "    X_train_scaled_survey[:, :, i] = survey_scalers[i].fit_transform(X_train_survey[:, :, i]) \n",
    "    \n",
    "activity_scalers = {}\n",
    "for i in range(n_features_activity):\n",
    "    activity_scalers[i] = MinMaxScaler()\n",
    "    X_train_scaled_activity[:, :, i] = activity_scalers[i].fit_transform(X_train_activity[:, :, i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1590/1590 [==============================] - 11s 7ms/step - loss: 1.3350 - accuracy: 0.9166 - auc_14: 0.8394 - recall_14: 0.6522 - precision_14: 0.0035 - val_loss: 0.0509 - val_accuracy: 0.9871 - val_auc_14: 0.9962 - val_recall_14: 1.0000 - val_precision_14: 0.0333\n",
      "Epoch 2/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 0.0514 - accuracy: 0.9885 - auc_14: 0.9990 - recall_14: 1.0000 - precision_14: 0.0380 - val_loss: 0.0229 - val_accuracy: 0.9938 - val_auc_14: 0.9972 - val_recall_14: 0.7500 - val_precision_14: 0.0517\n",
      "Epoch 3/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 0.0293 - accuracy: 0.9941 - auc_14: 0.9991 - recall_14: 1.0000 - precision_14: 0.0712 - val_loss: 0.0136 - val_accuracy: 0.9967 - val_auc_14: 0.9956 - val_recall_14: 0.7500 - val_precision_14: 0.0938\n",
      "Epoch 4/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 0.0387 - accuracy: 0.9950 - auc_14: 0.9990 - recall_14: 1.0000 - precision_14: 0.0824 - val_loss: 0.0137 - val_accuracy: 0.9965 - val_auc_14: 0.9975 - val_recall_14: 0.7500 - val_precision_14: 0.0909\n",
      "Epoch 5/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 0.0232 - accuracy: 0.9962 - auc_14: 0.9989 - recall_14: 1.0000 - precision_14: 0.1070 - val_loss: 0.0119 - val_accuracy: 0.9971 - val_auc_14: 0.8724 - val_recall_14: 0.5000 - val_precision_14: 0.0769\n",
      "Epoch 6/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 0.0413 - accuracy: 0.9954 - auc_14: 0.9988 - recall_14: 1.0000 - precision_14: 0.0898 - val_loss: 0.0130 - val_accuracy: 0.9964 - val_auc_14: 0.9977 - val_recall_14: 0.7500 - val_precision_14: 0.0882\n",
      "Epoch 7/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 0.0312 - accuracy: 0.9963 - auc_14: 0.9992 - recall_14: 1.0000 - precision_14: 0.1090 - val_loss: 0.0092 - val_accuracy: 0.9971 - val_auc_14: 0.8720 - val_recall_14: 0.7500 - val_precision_14: 0.1071\n",
      "Epoch 8/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 0.0196 - accuracy: 0.9969 - auc_14: 0.9992 - recall_14: 1.0000 - precision_14: 0.1285 - val_loss: 0.0080 - val_accuracy: 0.9982 - val_auc_14: 0.8710 - val_recall_14: 0.2500 - val_precision_14: 0.0714\n",
      "Epoch 9/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 0.0287 - accuracy: 0.9970 - auc_14: 0.9991 - recall_14: 1.0000 - precision_14: 0.1322 - val_loss: 0.0194 - val_accuracy: 0.9968 - val_auc_14: 0.8719 - val_recall_14: 0.7500 - val_precision_14: 0.0968\n",
      "Epoch 10/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 0.0199 - accuracy: 0.9973 - auc_14: 0.9991 - recall_14: 1.0000 - precision_14: 0.1420 - val_loss: 0.0134 - val_accuracy: 0.9977 - val_auc_14: 0.8728 - val_recall_14: 0.7500 - val_precision_14: 0.1304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f26918bd100>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train survey based model to learn repr.\n",
    "epochs = 10\n",
    "model_type = 'lstm'\n",
    "val_split = 0.15\n",
    "minority_weight = len(y_train)/sum(y_train)\n",
    "\n",
    "model_survey = create_model(model_type, n_timesteps_survey, n_features_survey)\n",
    "\n",
    "shuffled_X_train, shuffled_y_train = shuffle(X_train_scaled_survey, y_train, random_state = 42)\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode='min',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_survey.fit(shuffled_X_train, shuffled_y_train, class_weight = {1: minority_weight, 0: 1}, epochs=epochs, validation_split=val_split, callbacks = [early_stopping_monitor], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 2.3252 - accuracy: 0.6252 - auc_17: 0.3996 - recall_17: 0.2609 - precision_17: 3.1491e-04 - val_loss: 0.6989 - val_accuracy: 0.1310 - val_auc_17: 0.7416 - val_recall_17: 1.0000 - val_precision_17: 5.1256e-04\n",
      "Epoch 2/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 1.4067 - accuracy: 0.4977 - auc_17: 0.4559 - recall_17: 0.4348 - precision_17: 3.9145e-04 - val_loss: 0.6626 - val_accuracy: 0.8526 - val_auc_17: 0.7245 - val_recall_17: 0.5000 - val_precision_17: 0.0015\n",
      "Epoch 3/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 1.4187 - accuracy: 0.5601 - auc_17: 0.5290 - recall_17: 0.4348 - precision_17: 4.4697e-04 - val_loss: 0.7526 - val_accuracy: 0.0852 - val_auc_17: 0.7340 - val_recall_17: 1.0000 - val_precision_17: 4.8691e-04\n",
      "Epoch 4/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 1.3543 - accuracy: 0.4534 - auc_17: 0.6120 - recall_17: 0.6957 - precision_17: 5.7529e-04 - val_loss: 0.6575 - val_accuracy: 0.7255 - val_auc_17: 0.6811 - val_recall_17: 0.7500 - val_precision_17: 0.0012\n",
      "Epoch 5/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 1.3674 - accuracy: 0.6522 - auc_17: 0.6159 - recall_17: 0.4348 - precision_17: 5.6536e-04 - val_loss: 0.7038 - val_accuracy: 0.3644 - val_auc_17: 0.7472 - val_recall_17: 0.7500 - val_precision_17: 5.2567e-04\n",
      "Epoch 6/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 1.6450 - accuracy: 0.6173 - auc_17: 0.5826 - recall_17: 0.5217 - precision_17: 6.1646e-04 - val_loss: 0.5022 - val_accuracy: 0.9596 - val_auc_17: 0.7608 - val_recall_17: 0.2500 - val_precision_17: 0.0028\n",
      "Epoch 7/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 1.3886 - accuracy: 0.5588 - auc_17: 0.6576 - recall_17: 0.7391 - precision_17: 7.5714e-04 - val_loss: 0.6601 - val_accuracy: 0.6619 - val_auc_17: 0.7726 - val_recall_17: 0.7500 - val_precision_17: 9.8782e-04\n",
      "Epoch 8/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 1.4204 - accuracy: 0.5343 - auc_17: 0.7046 - recall_17: 0.8261 - precision_17: 8.0155e-04 - val_loss: 0.3873 - val_accuracy: 0.9957 - val_auc_17: 0.7492 - val_recall_17: 0.2500 - val_precision_17: 0.0270\n",
      "Epoch 9/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 1.2902 - accuracy: 0.7134 - auc_17: 0.6502 - recall_17: 0.4783 - precision_17: 7.5472e-04 - val_loss: 0.6229 - val_accuracy: 0.7695 - val_auc_17: 0.7615 - val_recall_17: 0.7500 - val_precision_17: 0.0014\n",
      "Epoch 10/10\n",
      "1590/1590 [==============================] - 10s 6ms/step - loss: 1.2023 - accuracy: 0.7525 - auc_17: 0.7687 - recall_17: 0.7826 - precision_17: 0.0014 - val_loss: 0.6567 - val_accuracy: 0.6755 - val_auc_17: 0.8226 - val_recall_17: 0.7500 - val_precision_17: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f26b18e65b0>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train activity based model to learn repr.\n",
    "model_activity  = create_model(model_type, n_timesteps_activity, n_features_activity)\n",
    "\n",
    "shuffled_X_train, shuffled_y_train = shuffle(X_train_scaled_activity, y_train, random_state = 42)\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=0,\n",
    "    mode='min',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_activity.fit(shuffled_X_train, shuffled_y_train, class_weight = {1: minority_weight, 0: 1}, epochs=epochs, validation_split=val_split, callbacks = [early_stopping_monitor], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get representations, mind the output layer\n",
    "trained_survey_model = keras.Model(inputs=model_survey.input, outputs=model_survey.layers[-2].output)\n",
    "survey_representations = trained_survey_model(X_train_scaled_survey).numpy()\n",
    "\n",
    "trained_activity_model = keras.Model(inputs=model_activity.input, outputs=model_activity.layers[-2].output)\n",
    "activity_representations = trained_activity_model(X_train_scaled_activity).numpy()\n",
    "\n",
    "#Scale survey representations\n",
    "survey_rep_scaler = MinMaxScaler()\n",
    "survey_representations_scaled = survey_rep_scaler.fit_transform(survey_representations)\n",
    "\n",
    "activity_rep_scaler = MinMaxScaler()\n",
    "activity_representations_scaled = activity_rep_scaler.fit_transform(activity_representations)\n",
    "\n",
    "#(n of windows, features) array participant baselines corresponding to each time window\n",
    "train_baseline = np.take(baseline_pca,train_b_ind,axis = 0)\n",
    "\n",
    "#Combine survey and baseline data into a single array\n",
    "train_combined = np.concatenate((activity_representations_scaled,survey_representations_scaled,train_baseline),axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.05, max_delta_step=0, max_depth=10,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=10, reg_lambda=1, scale_pos_weight=2216.296296296296,\n",
       "              subsample=1, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=0)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train end classifier\n",
    "#rfc = RandomForestClassifier(n_estimators=100000,max_depth=10)\n",
    "log = LogisticRegression(solver='lbfgs', class_weight='balanced',max_iter = 1000)\n",
    "xgb_classif = xgb.XGBClassifier(scale_pos_weight = minority_weight, n_estimators=100, colsample_bytree = 0.3, learning_rate = 0.05,max_depth = 10, alpha = 10, verbosity = 0)\n",
    "\n",
    "end_classifier = xgb_classif\n",
    "end_classifier.fit(train_combined,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survey pred based AUC:  0.6985628054038516\n",
      "survey proba based AUC:  0.9977004886461627\n",
      "[[3469   10]\n",
      " [   3    2]]\n",
      "activity pred based AUC:  0.4931014659384881\n",
      "activity proba based AUC:  0.6311008910606497\n",
      "[[3431   48]\n",
      " [   5    0]]\n",
      "combined pred based AUC:  0.6998562805403852\n",
      "combined proba based AUC:  0.9937338315607933\n",
      "[[3478    1]\n",
      " [   3    2]]\n",
      "survey pred based AUC:  0.7465190754664439\n",
      "survey proba based AUC:  0.9927596769702032\n",
      "[[3566   25]\n",
      " [   1    1]]\n",
      "activity pred based AUC:  0.7449874686716791\n",
      "activity proba based AUC:  0.5346700083542189\n",
      "[[3555   36]\n",
      " [   1    1]]\n",
      "combined pred based AUC:  0.7491645781119466\n",
      "combined proba based AUC:  0.7477722082985241\n",
      "[[3585    6]\n",
      " [   1    1]]\n",
      "survey pred based AUC:  0.6616312828887679\n",
      "survey proba based AUC:  0.988205407367084\n",
      "[[3637   37]\n",
      " [   2    1]]\n",
      "activity pred based AUC:  0.4974142623843223\n",
      "activity proba based AUC:  0.3478497550353838\n",
      "[[3655   19]\n",
      " [   3    0]]\n",
      "combined pred based AUC:  0.5\n",
      "combined proba based AUC:  0.6608600979858464\n",
      "[[3674    0]\n",
      " [   3    0]]\n",
      "survey pred based AUC:  0.9917545282508786\n",
      "survey proba based AUC:  0.9939172749391727\n",
      "[[3638   61]\n",
      " [   0    6]]\n",
      "activity pred based AUC:  0.49824276831576103\n",
      "activity proba based AUC:  0.6063801027304676\n",
      "[[3686   13]\n",
      " [   6    0]]\n",
      "combined pred based AUC:  0.5829278183292782\n",
      "combined proba based AUC:  0.9912138415788051\n",
      "[[3696    3]\n",
      " [   5    1]]\n",
      "survey pred based AUC:  0.9901729559748428\n",
      "survey proba based AUC:  0.9974668064290706\n",
      "[[3741   75]\n",
      " [   0    6]]\n",
      "activity pred based AUC:  0.4985587002096436\n",
      "activity proba based AUC:  0.4994758909853249\n",
      "[[3805   11]\n",
      " [   6    0]]\n",
      "combined pred based AUC:  0.748296645702306\n",
      "combined proba based AUC:  0.9976415094339622\n",
      "[[3803   13]\n",
      " [   3    3]]\n",
      "survey pred based AUC:  0.9911412193850965\n",
      "survey proba based AUC:  0.9946847316310579\n",
      "[[3770   68]\n",
      " [   0    5]]\n",
      "activity pred based AUC:  0.5963522668056279\n",
      "activity proba based AUC:  0.520401250651381\n",
      "[[3810   28]\n",
      " [   4    1]]\n",
      "combined pred based AUC:  0.8984366857738406\n",
      "combined proba based AUC:  0.9900990099009901\n",
      "[[3826   12]\n",
      " [   1    4]]\n",
      "survey pred based AUC:  0.9907834101382488\n",
      "survey proba based AUC:  0.9975678443420378\n",
      "[[3834   72]\n",
      " [   0    4]]\n",
      "activity pred based AUC:  0.39061699948796724\n",
      "activity proba based AUC:  0.45942140296979006\n",
      "[[ 122 3784]\n",
      " [   1    3]]\n",
      "combined pred based AUC:  0.6234639016897081\n",
      "combined proba based AUC:  0.9934075780849975\n",
      "[[3894   12]\n",
      " [   3    1]]\n",
      "survey pred based AUC:  0.9930773249738767\n",
      "survey proba based AUC:  0.9977359804946012\n",
      "[[3775   53]\n",
      " [   0    6]]\n",
      "activity pred based AUC:  0.5048328108672937\n",
      "activity proba based AUC:  0.33435649599442696\n",
      "[[  37 3791]\n",
      " [   0    6]]\n",
      "combined pred based AUC:  0.6666666666666666\n",
      "combined proba based AUC:  0.997561825148032\n",
      "[[3828    0]\n",
      " [   4    2]]\n",
      "survey pred based AUC:  0.9921754746023601\n",
      "survey proba based AUC:  0.9991662390969728\n",
      "[[3837   61]\n",
      " [   0    4]]\n",
      "activity pred based AUC:  0.5056439199589533\n",
      "activity proba based AUC:  0.5170920985120574\n",
      "[[  44 3854]\n",
      " [   0    4]]\n",
      "combined pred based AUC:  0.9989738327347358\n",
      "combined proba based AUC:  0.9992945100051309\n",
      "[[3890    8]\n",
      " [   0    4]]\n",
      "\n",
      " Average survey pred AUC:  0.8950908974538186\n",
      "\n",
      " Average survey prob AUC:  0.9954671611018182\n",
      "\n",
      " Overall survey accuracy:  0.9861415457506663\n",
      "\n",
      " Overall survey precision:  0.07042253521126761\n",
      "\n",
      " Overall survey recall:  0.8536585365853658\n",
      "\n",
      " Overall survey f-1 score:  0.13011152416356878\n",
      "\n",
      " Average activity pred AUC:  0.5255278514044152\n",
      "\n",
      " Average activity prob AUC:  0.4945275440326334\n",
      "\n",
      " Overall activity accuracy:  0.6562037311222979\n",
      "\n",
      " Overall activity precision:  0.001293214932321752\n",
      "\n",
      " Overall activity recall:  0.36585365853658536\n",
      "\n",
      " Overall activity f-1 score:  0.002577319587628866\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_auc_preds_combined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-1d3f9dde47c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0mcombined_f1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcombined_precision\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcombined_recall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_precision\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcombined_recall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n Average pred combined AUC: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_auc_preds_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_auc_preds_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n Average prob combined AUC: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_auc_prob_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_auc_prob_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'avg_auc_preds_combined' is not defined"
     ]
    }
   ],
   "source": [
    "#Train & valid loop\n",
    "avg_survey_auc_preds = []\n",
    "avg_survey_auc_prob = []\n",
    "\n",
    "survey_TN = 0\n",
    "survey_FN = 0\n",
    "survey_TP = 0\n",
    "survey_FP = 0\n",
    "\n",
    "avg_activity_auc_preds = []\n",
    "avg_activity_auc_prob = []\n",
    "\n",
    "activity_TN = 0\n",
    "activity_FN = 0\n",
    "activity_TP = 0\n",
    "activity_FP = 0\n",
    "\n",
    "avg_combined_auc_preds = []\n",
    "avg_combined_auc_prob = []\n",
    "\n",
    "combined_TN = 0\n",
    "combined_FN = 0\n",
    "combined_TP = 0\n",
    "combined_FP = 0\n",
    "\n",
    "#Scaling issue!\n",
    "survey_scalers = {}\n",
    "for i in range(n_features_survey):\n",
    "    survey_scalers[i] = MinMaxScaler()\n",
    "    X_train_scaled_survey[:, :, i] = survey_scalers[i].fit_transform(X_train_survey[:, :, i]) \n",
    "    \n",
    "activity_scalers = {}\n",
    "for i in range(n_features_activity):\n",
    "    activity_scalers[i] = MinMaxScaler()\n",
    "    X_train_scaled_activity[:, :, i] = activity_scalers[i].fit_transform(X_train_activity[:, :, i]) \n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(validation_survey_x_list)):\n",
    "    \n",
    "    #Predict\n",
    "    daily_survey_batch_X = validation_survey_x_list[i]\n",
    "    daily_activity_batch_X = validation_activity_x_list[i]    \n",
    "    daily_batch_y = validation_y_list[i]\n",
    "    \n",
    "    scaled_daily_survey_batch_X = daily_survey_batch_X\n",
    "    for feat in range(n_features_survey):\n",
    "        scaled_daily_survey_batch_X[:, :, feat] = survey_scalers[feat].transform(daily_survey_batch_X[:, :, feat]) \n",
    "\n",
    "    scaled_daily_activity_batch_X = daily_activity_batch_X\n",
    "    for feat in range(n_features_activity):\n",
    "        scaled_daily_activity_batch_X[:, :, feat] = activity_scalers[feat].transform(daily_activity_batch_X[:, :, feat])\n",
    "    \n",
    "    #survey based predictions\n",
    "    survey_preds = model_survey.predict_classes(scaled_daily_survey_batch_X, verbose=0)\n",
    "    survey_prob = model_survey.predict(scaled_daily_survey_batch_X, verbose=0)\n",
    "    \n",
    "    roc_auc_preds = roc_auc_score(daily_batch_y,survey_preds)\n",
    "    roc_auc_prob = roc_auc_score(daily_batch_y,survey_prob)\n",
    "        \n",
    "    avg_survey_auc_preds += [roc_auc_preds]\n",
    "    avg_survey_auc_prob += [roc_auc_prob]\n",
    "  \n",
    "    survey_CM = confusion_matrix(daily_batch_y, survey_preds)\n",
    "\n",
    "    survey_TN += survey_CM[0][0]\n",
    "    survey_FN += survey_CM[1][0]\n",
    "    survey_TP += survey_CM[1][1]\n",
    "    survey_FP += survey_CM[0][1]\n",
    "\n",
    "    print(\"survey pred based AUC: \", roc_auc_preds)\n",
    "    print(\"survey proba based AUC: \", roc_auc_prob)\n",
    "    print(survey_CM)\n",
    "    \n",
    "    #activity based predictions\n",
    "    activity_preds = model_activity.predict_classes(scaled_daily_activity_batch_X, verbose=0)\n",
    "    activity_prob = model_activity.predict(scaled_daily_activity_batch_X, verbose=0)\n",
    "    \n",
    "    roc_auc_preds = roc_auc_score(daily_batch_y,activity_preds)\n",
    "    roc_auc_prob = roc_auc_score(daily_batch_y,activity_prob)\n",
    "        \n",
    "    avg_activity_auc_preds += [roc_auc_preds]\n",
    "    avg_activity_auc_prob += [roc_auc_prob]\n",
    "  \n",
    "    activity_CM = confusion_matrix(daily_batch_y, activity_preds)\n",
    "\n",
    "    activity_TN += activity_CM[0][0]\n",
    "    activity_FN += activity_CM[1][0]\n",
    "    activity_TP += activity_CM[1][1]\n",
    "    activity_FP += activity_CM[0][1]\n",
    "\n",
    "    print(\"activity pred based AUC: \", roc_auc_preds)\n",
    "    print(\"activity proba based AUC: \", roc_auc_prob)\n",
    "    print(activity_CM)\n",
    "\n",
    "    \n",
    "    \n",
    "    #Get representations, mind the output layer\n",
    "    survey_rep_model = keras.Model(inputs=model_survey.input, outputs=model_survey.layers[-2].output)\n",
    "    daily_survey_representations = survey_rep_model(scaled_daily_survey_batch_X).numpy()\n",
    "    \n",
    "    activity_rep_model = keras.Model(inputs=model_activity.input, outputs=model_activity.layers[-2].output)\n",
    "    daily_activity_representations = activity_rep_model(scaled_daily_activity_batch_X).numpy()\n",
    "\n",
    "    #Scaled representations\n",
    "    daily_survey_representations_scaled = survey_rep_scaler.transform(daily_survey_representations)\n",
    "    daily_activity_representations_scaled = activity_rep_scaler.transform(daily_activity_representations)\n",
    "\n",
    "\n",
    "    #(n of windows, features) array participant baselines corresponding to each time window\n",
    "    daily_baseline = np.take(baseline_pca,valid_b_ind[i],axis = 0)\n",
    "\n",
    "    #Combine survey and baseline data into a single array\n",
    "    daily_combined = np.concatenate((daily_activity_representations_scaled,daily_survey_representations_scaled,daily_baseline),axis = 1)\n",
    "    \n",
    "    #Classify\n",
    "    combined_preds = end_classifier.predict(daily_combined)\n",
    "    combined_prob = end_classifier.predict_proba(daily_combined)\n",
    "\n",
    "    roc_auc_preds = roc_auc_score(daily_batch_y,combined_preds)\n",
    "    roc_auc_prob = roc_auc_score(daily_batch_y,combined_prob[:,1])\n",
    " \n",
    "    avg_combined_auc_preds += [roc_auc_preds]\n",
    "    avg_combined_auc_prob += [roc_auc_prob]\n",
    "   \n",
    "    combined_CM = confusion_matrix(daily_batch_y, combined_preds)\n",
    "\n",
    "    combined_TN += combined_CM[0][0]\n",
    "    combined_FN += combined_CM[1][0]\n",
    "    combined_TP += combined_CM[1][1]\n",
    "    combined_FP += combined_CM[0][1]\n",
    "   \n",
    "    print(\"combined pred based AUC: \", roc_auc_preds)\n",
    "    print(\"combined proba based AUC: \", roc_auc_prob)\n",
    "    print(combined_CM) \n",
    "    \n",
    "    #Train\n",
    "    #X_train, y_train = shuffle(np.concatenate((X_train,daily_batch_X)), np.append(y_train,daily_batch_y), random_state = 42)\n",
    "    \n",
    "    \"\"\"\n",
    "    scalers = {}\n",
    "    for j in range(n_features):\n",
    "        scalers[j] = StandardScaler()\n",
    "        scalers[j].fit(X_train[:, :, j]) \n",
    "        scaled_daily_batch_X[:, :, j] = scalers[j].transform(daily_batch_X[:, :, j])     \n",
    "    \"\"\"\n",
    "    shuffled_X, shuffled_y= shuffle(scaled_daily_survey_batch_X, daily_batch_y, random_state = 42)\n",
    "    model_survey.fit(shuffled_X, shuffled_y, class_weight = {1: minority_weight, 0: 1}, epochs=1, verbose=0)\n",
    "\n",
    "    shuffled_X, shuffled_y= shuffle(scaled_daily_activity_batch_X, daily_batch_y, random_state = 42)\n",
    "    model_activity.fit(shuffled_X, shuffled_y, class_weight = {1: minority_weight, 0: 1}, epochs=1, verbose=0)\n",
    "    \n",
    "    #Get representations, mind the output layer\n",
    "    survey_rep_model = keras.Model(inputs=model_survey.input, outputs=model_survey.layers[-2].output)\n",
    "    daily_survey_representations = survey_rep_model(scaled_daily_survey_batch_X).numpy()\n",
    "    \n",
    "    activity_rep_model = keras.Model(inputs=model_activity.input, outputs=model_activity.layers[-2].output)\n",
    "    daily_activity_representations = activity_rep_model(scaled_daily_activity_batch_X).numpy()\n",
    "\n",
    "    #Scaled representations\n",
    "    daily_survey_representations_scaled = survey_rep_scaler.transform(daily_survey_representations)\n",
    "    daily_activity_representations_scaled = activity_rep_scaler.transform(daily_activity_representations)\n",
    "    \n",
    "\n",
    "    #Combine survey and baseline data into a single array\n",
    "    daily_combined = np.concatenate((daily_activity_representations_scaled,daily_survey_representations_scaled,daily_baseline),axis = 1)\n",
    "    \n",
    "    #Train end-classif.\n",
    "    end_classifier.fit(daily_combined, daily_batch_y)\n",
    "'''#Train & valid loop\n",
    "avg_survey_auc_preds = []\n",
    "avg_survey_auc_prob = []\n",
    "\n",
    "survey_TN = 0\n",
    "survey_FN = 0\n",
    "survey_TP = 0\n",
    "survey_FP = 0\n",
    "\n",
    "avg_activity_auc_preds = []\n",
    "avg_activity_auc_prob = []\n",
    "\n",
    "activity_TN = 0\n",
    "activity_FN = 0\n",
    "activity_TP = 0\n",
    "activity_FP = 0\n",
    "\n",
    "avg_combined_auc_preds = []\n",
    "avg_combined_auc_prob = []\n",
    "\n",
    "combined_TN = 0\n",
    "combined_FN = 0\n",
    "combined_TP = 0\n",
    "combined_FP = 0'''\n",
    "\n",
    "survey_acc = (survey_TP + survey_TN) / (survey_TP + survey_TN + survey_FP + survey_FN)\n",
    "survey_precision = survey_TP/(survey_TP+survey_FP)\n",
    "survey_recall = survey_TP/(survey_TP+survey_FN)\n",
    "survey_f1_score = (2*survey_precision*survey_recall)/(survey_precision+survey_recall)\n",
    "\n",
    "print(\"\\n Average survey pred AUC: \", sum(avg_survey_auc_preds)/len(avg_survey_auc_preds))\n",
    "print(\"\\n Average survey prob AUC: \", sum(avg_survey_auc_prob)/len(avg_survey_auc_prob))\n",
    "\n",
    "print(\"\\n Overall survey accuracy: \", survey_acc)    \n",
    "print(\"\\n Overall survey precision: \", survey_precision)    \n",
    "print(\"\\n Overall survey recall: \", survey_recall)    \n",
    "print(\"\\n Overall survey f-1 score: \", survey_f1_score)   \n",
    "\n",
    "activity_acc = (activity_TP + activity_TN) / (activity_TP + activity_TN + activity_FP + activity_FN)\n",
    "activity_precision = activity_TP/(activity_TP+activity_FP)\n",
    "activity_recall = activity_TP/(activity_TP+activity_FN)\n",
    "activity_f1_score = (2*activity_precision*activity_recall)/(activity_precision+activity_recall)\n",
    "\n",
    "print(\"\\n Average activity pred AUC: \", sum(avg_activity_auc_preds)/len(avg_activity_auc_preds))\n",
    "print(\"\\n Average activity prob AUC: \", sum(avg_activity_auc_prob)/len(avg_activity_auc_prob))\n",
    "\n",
    "print(\"\\n Overall activity accuracy: \", activity_acc)    \n",
    "print(\"\\n Overall activity precision: \", activity_precision)    \n",
    "print(\"\\n Overall activity recall: \", activity_recall)    \n",
    "print(\"\\n Overall activity f-1 score: \", activity_f1_score) \n",
    "\n",
    "combined_acc = (combined_TP + combined_TN) / (combined_TP + combined_TN + combined_FP + combined_FN)\n",
    "combined_precision = combined_TP/(combined_TP+combined_FP)\n",
    "combined_recall = combined_TP/(combined_TP+combined_FN)\n",
    "combined_f1_score = (2*combined_precision*combined_recall)/(combined_precision+combined_recall)\n",
    "\n",
    "print(\"\\n Average pred combined AUC: \", sum(avg_combined_auc_preds)/len(avg_combined_auc_preds))    \n",
    "print(\"\\n Average prob combined AUC: \", sum(avg_combined_auc_prob)/len(avg_combined_auc_prob))    \n",
    "\n",
    "print(\"\\n Overall combined accuracy: \", combined_acc)    \n",
    "print(\"\\n Overall combined precision: \", combined_precision)    \n",
    "print(\"\\n Overall combined recall: \", combined_recall)    \n",
    "print(\"\\n Overall combined f-1 score: \", combined_f1_score)    \n",
    "\n",
    "print('\\n Survey conf mat\\n',survey_TN,'|',survey_FP,'\\n',survey_FN,'|',survey_TP,'\\n')\n",
    "print('\\n Activity conf mat\\n',activity_TN,'|',activityy_FP,'\\n',activity_FN,'|',activity_TP,'\\n')\n",
    "print('\\n Combined conf mat\\n',combined_TN,'|',combined_FP,'\\n',combined_FN,'|',combined_TP,'\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
